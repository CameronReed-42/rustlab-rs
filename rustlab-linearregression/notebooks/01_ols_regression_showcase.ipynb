{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares (OLS) Regression Showcase\n",
    "\n",
    "This notebook demonstrates the capabilities of RustLab's OLS linear regression implementation, including:\n",
    "- Basic linear regression with single and multiple features\n",
    "- Statistical inference (p-values, confidence intervals)\n",
    "- Model diagnostics (R², residual analysis)\n",
    "- Visualization of results\n",
    "- Comparison with and without intercept\n",
    "- Feature normalization effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dependencies loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "// Setup: Load dependencies\n",
    ":dep rustlab-math = { path = \"../../rustlab-math\" }\n",
    ":dep rustlab-linearregression = { path = \"..\" }\n",
    ":dep rustlab-distributions = { path = \"../../rustlab-distributions\" }\n",
    ":dep rand = \"0.8\"\n",
    "\n",
    "use rustlab_linearregression::prelude::*;\n",
    "use rustlab_math::{array64, vec64, ArrayF64, VectorF64, linspace, BasicStatistics};\n",
    "use rustlab_distributions::continuous::Normal;\n",
    "use rand::Rng;\n",
    "use std::f64::consts::PI;\n",
    "\n",
    "println!(\"✅ Dependencies loaded successfully!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Linear Regression (Single Feature)\n",
    "\n",
    "Let's start with a simple example: predicting house prices based on square footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Simple Linear Regression Results\n",
      "=====================================\n",
      "\n",
      "True model: price = 50000 + 150.0 × sqft\n",
      "Fitted model: price = 42116177 + -27780.9 × sqft\n",
      "\n",
      "📈 Model Performance:\n",
      "R² = 0.0098\n",
      "Adjusted R² = -0.0108\n",
      "\n",
      "📊 Statistical Significance:\n",
      "Slope p-value: 0.492405\n",
      "\n",
      "📋 Sample Data Points:\n",
      "  1000sqft → $195725 (predicted: $14335300)\n",
      "  1040.8163265306123sqft → $358075612 (predicted: $13201387)\n",
      "  1081.6326530612246sqft → $259495 (predicted: $12067474)\n",
      "  1122.4489795918366sqft → $201109919 (predicted: $10933560)\n",
      "  1163.265306122449sqft → $215041 (predicted: $9799647)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Generate synthetic data with linear relationship + noise\n",
    "{\n",
    "    let n_samples = 50;\n",
    "    let mut rng = rand::thread_rng();\n",
    "    let noise_dist = Normal::new(0.0, 15000.0).unwrap();\n",
    "    \n",
    "    // Generate square footage (1000 to 3000 sq ft)\n",
    "    let sqft = linspace(1000.0, 3000.0, n_samples);\n",
    "    \n",
    "    // True relationship: price = 50000 + 150 * sqft + noise\n",
    "    let true_intercept = 50000.0;\n",
    "    let true_slope = 150.0;\n",
    "    \n",
    "    let mut prices = VectorF64::zeros(n_samples);\n",
    "    for i in 0..n_samples {\n",
    "        let noise = noise_dist.sample(&mut rng);\n",
    "        prices[i] = true_intercept + true_slope * sqft[i] + noise;\n",
    "    }\n",
    "    \n",
    "    // Convert to feature matrix (n × 1)\n",
    "    let X = ArrayF64::from_vector_column(&sqft);\n",
    "    \n",
    "    // Fit OLS model\n",
    "    let model = LinearRegression::new();\n",
    "    let fitted = model.fit(&X, &prices).unwrap();\n",
    "    \n",
    "    // Get predictions\n",
    "    let predictions = fitted.predict(&X);\n",
    "    \n",
    "    // Display results\n",
    "    println!(\"📊 Simple Linear Regression Results\");\n",
    "    println!(\"=====================================\\n\");\n",
    "    println!(\"True model: price = {:.0} + {:.1} × sqft\", true_intercept, true_slope);\n",
    "    println!(\"Fitted model: price = {:.0} + {:.1} × sqft\", \n",
    "             fitted.intercept().unwrap(), \n",
    "             fitted.coefficients()[0]);\n",
    "    println!(\"\\n📈 Model Performance:\");\n",
    "    println!(\"R² = {:.4}\", fitted.r_squared());\n",
    "    println!(\"Adjusted R² = {:.4}\", fitted.adjusted_r_squared());\n",
    "    \n",
    "    // Statistical inference\n",
    "    if let Some(p_values) = fitted.p_values() {\n",
    "        println!(\"\\n📊 Statistical Significance:\");\n",
    "        println!(\"Slope p-value: {:.6}\", p_values[0]);\n",
    "        if p_values[0] < 0.05 {\n",
    "            println!(\"✅ Slope is statistically significant (p < 0.05)\");\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Show some data points for verification\n",
    "    println!(\"\\n📋 Sample Data Points:\");\n",
    "    for i in 0..5 {\n",
    "        println!(\"  {}sqft → ${:.0} (predicted: ${:.0})\", \n",
    "                 sqft[i], prices[i], predictions[i]);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Residual Analysis\n",
    "\n",
    "Examining residuals helps validate model assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Residual Analysis:\n",
      "===================\n",
      "Mean of residuals: 0.00 (should be ≈ 0)\n",
      "Std of residuals: 156664413.92\n",
      "Min residual: $-365193055\n",
      "Max residual: $346548252\n",
      "\n",
      "📋 Residuals by House Size:\n",
      "  1000sqft: residual = $48798985\n",
      "  1490sqft: residual = $42376470\n",
      "  2020sqft: residual = $-260364159\n",
      "  2510sqft: residual = $109285215\n",
      "  3000sqft: residual = $-154321889\n",
      "\n",
      "Mean absolute residual: $110797940\n",
      "✅ Residuals appear reasonably distributed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Residual analysis for the simple regression\n",
    "{\n",
    "    // Recreate the data\n",
    "    let n_samples = 50;\n",
    "    let mut rng = rand::thread_rng();\n",
    "    let noise_dist = Normal::new(0.0, 15000.0).unwrap();\n",
    "    \n",
    "    let sqft = linspace(1000.0, 3000.0, n_samples);\n",
    "    let mut prices = VectorF64::zeros(n_samples);\n",
    "    for i in 0..n_samples {\n",
    "        let noise = noise_dist.sample(&mut rng);\n",
    "        prices[i] = 50000.0 + 150.0 * sqft[i] + noise;\n",
    "    }\n",
    "    \n",
    "    let X = ArrayF64::from_vector_column(&sqft);\n",
    "    let fitted = LinearRegression::new().fit(&X, &prices).unwrap();\n",
    "    let predictions = fitted.predict(&X);\n",
    "    \n",
    "    // Get residuals\n",
    "    let residuals = if let Some(res) = fitted.residuals() {\n",
    "        res.clone()\n",
    "    } else {\n",
    "        &prices - &predictions\n",
    "    };\n",
    "    \n",
    "    println!(\"\\n📊 Residual Analysis:\");\n",
    "    println!(\"===================\");\n",
    "    println!(\"Mean of residuals: {:.2} (should be ≈ 0)\", residuals.mean());\n",
    "    println!(\"Std of residuals: {:.2}\", residuals.std(None));\n",
    "    \n",
    "    // Check for patterns in residuals\n",
    "    let sorted_residuals = {\n",
    "        let mut res_vec: Vec<f64> = residuals.iter().cloned().collect();\n",
    "        res_vec.sort_by(|a, b| a.partial_cmp(b).unwrap());\n",
    "        res_vec\n",
    "    };\n",
    "    \n",
    "    println!(\"Min residual: ${:.0}\", sorted_residuals[0]);\n",
    "    println!(\"Max residual: ${:.0}\", sorted_residuals[sorted_residuals.len() - 1]);\n",
    "    \n",
    "    // Show residuals for different house sizes\n",
    "    println!(\"\\n📋 Residuals by House Size:\");\n",
    "    let indices = vec![0, 12, 25, 37, 49]; // Different house sizes\n",
    "    for &i in indices.iter() {\n",
    "        println!(\"  {:.0}sqft: residual = ${:.0}\", sqft[i], residuals[i]);\n",
    "    }\n",
    "    \n",
    "    // Check normality assumption (simplified)\n",
    "    let abs_residuals: Vec<f64> = residuals.iter().map(|&r| r.abs()).collect();\n",
    "    let mean_abs_residual = abs_residuals.iter().sum::<f64>() / abs_residuals.len() as f64;\n",
    "    println!(\"\\nMean absolute residual: ${:.0}\", mean_abs_residual);\n",
    "    \n",
    "    if mean_abs_residual < residuals.std(None) {\n",
    "        println!(\"✅ Residuals appear reasonably distributed\");\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiple Linear Regression\n",
    "\n",
    "Now let's use multiple features to predict outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Multiple Linear Regression Results\n",
      "======================================\n",
      "\n",
      "True coefficients:\n",
      "  Intercept: 40000\n",
      "  Sqft:      120.0\n",
      "  Bedrooms:  5000\n",
      "  Age:       -1000\n",
      "\n",
      "Fitted coefficients (raw features):\n",
      "  Intercept: 997420917\n",
      "  Sqft:      -909468.4\n",
      "  Bedrooms:  -42505433\n",
      "  Age:       64628887\n",
      "\n",
      "📈 Model Performance:\n",
      "R² (raw):        -0.2054\n",
      "R² (normalized): -0.0092\n",
      "Adjusted R²:     -0.2430\n",
      "\n",
      "📊 Feature Importance (normalized model):\n",
      "  Bedrooms: 30207295.725\n",
      "  Sqft: 26681294.005\n",
      "  Age: 12612490.045\n",
      "\n",
      "📊 Statistical Significance (p-values):\n",
      "  Sqft: 0.000000 ***\n",
      "  Bedrooms: 0.307280 \n",
      "  Age: 0.000000 ***\n",
      "\n",
      "📋 Sample Predictions:\n",
      "  1000sqft, 3bed, 0yrs → $-39563828 (actual: $185041)\n",
      "  1404sqft, 3bed, 6yrs → $-15335605 (actual: $211878)\n",
      "  1808sqft, 4bed, 12yrs → $-33612815 (actual: $256681)\n",
      "  2212sqft, 4bed, 18yrs → $-9384592 (actual: $301620)\n",
      "  2616sqft, 5bed, 24yrs → $-27661802 (actual: $354128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Multiple regression: House prices with 3 features\n",
    "{\n",
    "    let n_samples = 100;\n",
    "    let mut rng = rand::thread_rng();\n",
    "    let noise_dist = Normal::new(0.0, 10000.0).unwrap();\n",
    "    \n",
    "    // Generate features\n",
    "    let sqft = linspace(1000.0, 3000.0, n_samples);\n",
    "    let bedrooms = {\n",
    "        let mut b = VectorF64::zeros(n_samples);\n",
    "        for i in 0..n_samples {\n",
    "            b[i] = 2.0 + (sqft[i] / 1000.0).round();\n",
    "        }\n",
    "        b\n",
    "    };\n",
    "    let age = linspace(0.0, 30.0, n_samples);\n",
    "    \n",
    "    // Create feature matrix (n × 3) manually\n",
    "    let mut X = ArrayF64::zeros(n_samples, 3);\n",
    "    for i in 0..n_samples {\n",
    "        X[(i, 0)] = sqft[i];\n",
    "        X[(i, 1)] = bedrooms[i];\n",
    "        X[(i, 2)] = age[i];\n",
    "    }\n",
    "    \n",
    "    // True relationship: price = 40000 + 120*sqft + 5000*bedrooms - 1000*age + noise\n",
    "    let true_coef = vec64![120.0, 5000.0, -1000.0];\n",
    "    let true_intercept = 40000.0;\n",
    "    \n",
    "    let mut y = VectorF64::zeros(n_samples);\n",
    "    for i in 0..n_samples {\n",
    "        let noise = noise_dist.sample(&mut rng);\n",
    "        y[i] = true_intercept \n",
    "            + true_coef[0] * sqft[i]\n",
    "            + true_coef[1] * bedrooms[i]\n",
    "            + true_coef[2] * age[i]\n",
    "            + noise;\n",
    "    }\n",
    "    \n",
    "    // Fit model with and without normalization\n",
    "    let model_raw = LinearRegression::new()\n",
    "        .with_normalization(false);\n",
    "    let model_norm = LinearRegression::new()\n",
    "        .with_normalization(true);\n",
    "    \n",
    "    let fitted_raw = model_raw.fit(&X, &y).unwrap();\n",
    "    let fitted_norm = model_norm.fit(&X, &y).unwrap();\n",
    "    \n",
    "    println!(\"📊 Multiple Linear Regression Results\");\n",
    "    println!(\"======================================\\n\");\n",
    "    \n",
    "    println!(\"True coefficients:\");\n",
    "    println!(\"  Intercept: {:.0}\", true_intercept);\n",
    "    println!(\"  Sqft:      {:.1}\", true_coef[0]);\n",
    "    println!(\"  Bedrooms:  {:.0}\", true_coef[1]);\n",
    "    println!(\"  Age:       {:.0}\\n\", true_coef[2]);\n",
    "    \n",
    "    println!(\"Fitted coefficients (raw features):\");\n",
    "    println!(\"  Intercept: {:.0}\", fitted_raw.intercept().unwrap());\n",
    "    let coef_raw = fitted_raw.coefficients();\n",
    "    println!(\"  Sqft:      {:.1}\", coef_raw[0]);\n",
    "    println!(\"  Bedrooms:  {:.0}\", coef_raw[1]);\n",
    "    println!(\"  Age:       {:.0}\\n\", coef_raw[2]);\n",
    "    \n",
    "    println!(\"📈 Model Performance:\");\n",
    "    println!(\"R² (raw):        {:.4}\", fitted_raw.r_squared());\n",
    "    println!(\"R² (normalized): {:.4}\", fitted_norm.r_squared());\n",
    "    println!(\"Adjusted R²:     {:.4}\\n\", fitted_raw.adjusted_r_squared());\n",
    "    \n",
    "    // Feature importance via standardized coefficients\n",
    "    println!(\"📊 Feature Importance (normalized model):\");\n",
    "    let coef_norm = fitted_norm.coefficients();\n",
    "    let features = vec![\"Sqft\", \"Bedrooms\", \"Age\"];\n",
    "    \n",
    "    let mut importance: Vec<(String, f64)> = features.iter()\n",
    "        .zip(coef_norm.iter())\n",
    "        .map(|(name, &coef)| (name.to_string(), coef.abs()))\n",
    "        .collect();\n",
    "    \n",
    "    importance.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());\n",
    "    \n",
    "    for (name, imp) in importance.iter() {\n",
    "        println!(\"  {}: {:.3}\", name, imp);\n",
    "    }\n",
    "    \n",
    "    // Statistical significance\n",
    "    if let Some(p_values) = fitted_raw.p_values() {\n",
    "        println!(\"\\n📊 Statistical Significance (p-values):\");\n",
    "        for (i, &p) in p_values.iter().enumerate() {\n",
    "            let sig = if p < 0.001 { \"***\" }\n",
    "                     else if p < 0.01 { \"**\" }\n",
    "                     else if p < 0.05 { \"*\" }\n",
    "                     else { \"\" };\n",
    "            println!(\"  {}: {:.6} {}\", features[i], p, sig);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Show a few predictions\n",
    "    println!(\"\\n📋 Sample Predictions:\");\n",
    "    let predictions = fitted_raw.predict(&X);\n",
    "    for i in (0..n_samples).step_by(20) {\n",
    "        println!(\"  {:.0}sqft, {:.0}bed, {:.0}yrs → ${:.0} (actual: ${:.0})\", \n",
    "                 sqft[i], bedrooms[i], age[i], predictions[i], y[i]);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confidence Intervals for Predictions\n",
    "\n",
    "OLS provides confidence intervals to quantify prediction uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Confidence Intervals Analysis\n",
      "=================================\n",
      "\n",
      "Model Summary:\n",
      "R² = 0.9945\n",
      "Number of features: 3\n",
      "Number of training samples: 30\n",
      "\n",
      "📋 Predictions with 95% Confidence Intervals:\n",
      "X Value | Prediction | Lower CI | Upper CI | Interval Width\n",
      "--------|------------|----------|----------|---------------\n",
      "  -0.50 |     -1.475 |   -7.905 |    4.955 |        12.860\n",
      "   1.26 |      3.521 |   -2.909 |    9.951 |        12.860\n",
      "   3.02 |      6.177 |   -0.253 |   12.607 |        12.860\n",
      "   4.78 |      8.579 |    2.149 |   15.009 |        12.860\n",
      "   6.53 |     13.416 |    6.986 |   19.846 |        12.860\n",
      "\n",
      "📋 Training Data Fit:\n",
      "x = 0.00: actual = -0.309, predicted = 0.026, residual = -0.335\n",
      "x = 1.30: actual = 3.875, predicted = 3.615, residual = 0.260\n",
      "x = 2.60: actual = 5.974, predicted = 5.744, residual = 0.230\n",
      "x = 3.90: actual = 6.995, predicted = 7.121, residual = -0.126\n",
      "x = 5.20: actual = 9.444, predicted = 9.557, residual = -0.113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Confidence intervals demonstration\n",
    "{\n",
    "    // Generate data with known relationship\n",
    "    let n_train = 30;\n",
    "    let mut rng = rand::thread_rng();\n",
    "    let noise_dist = Normal::new(0.0, 0.3).unwrap();\n",
    "    \n",
    "    // Training data\n",
    "    let x_train = linspace(0.0, 2.0 * PI, n_train);\n",
    "    let mut y_train = VectorF64::zeros(n_train);\n",
    "    for i in 0..n_train {\n",
    "        let noise = noise_dist.sample(&mut rng);\n",
    "        y_train[i] = 2.0 * x_train[i] + x_train[i].sin() + noise;\n",
    "    }\n",
    "    \n",
    "    // Create polynomial features for better fit (manually)\n",
    "    let mut X_train = ArrayF64::zeros(n_train, 3);\n",
    "    for i in 0..n_train {\n",
    "        X_train[(i, 0)] = x_train[i];                // Linear term\n",
    "        X_train[(i, 1)] = x_train[i].sin();          // Sin term\n",
    "        X_train[(i, 2)] = x_train[i].cos();          // Cos term\n",
    "    }\n",
    "    \n",
    "    // Fit model\n",
    "    let model = LinearRegression::new();\n",
    "    let fitted = model.fit(&X_train, &y_train).unwrap();\n",
    "    \n",
    "    // Test data for smooth prediction curve\n",
    "    let n_test = 20;\n",
    "    let x_test = linspace(-0.5, 2.5 * PI, n_test);\n",
    "    let mut X_test = ArrayF64::zeros(n_test, 3);\n",
    "    for i in 0..n_test {\n",
    "        X_test[(i, 0)] = x_test[i];\n",
    "        X_test[(i, 1)] = x_test[i].sin();\n",
    "        X_test[(i, 2)] = x_test[i].cos();\n",
    "    }\n",
    "    \n",
    "    // Get predictions with confidence intervals\n",
    "    let (predictions, lower, upper) = fitted.predict_interval(&X_test, 0.05).unwrap();\n",
    "    \n",
    "    println!(\"📊 Confidence Intervals Analysis\");\n",
    "    println!(\"=================================\\n\");\n",
    "    \n",
    "    println!(\"Model Summary:\");\n",
    "    println!(\"R² = {:.4}\", fitted.r_squared());\n",
    "    println!(\"Number of features: {}\", fitted.n_features());\n",
    "    println!(\"Number of training samples: {}\\n\", fitted.n_samples());\n",
    "    \n",
    "    println!(\"📋 Predictions with 95% Confidence Intervals:\");\n",
    "    println!(\"X Value | Prediction | Lower CI | Upper CI | Interval Width\");\n",
    "    println!(\"--------|------------|----------|----------|---------------\");\n",
    "    \n",
    "    for i in (0..n_test).step_by(4) {\n",
    "        let width = upper[i] - lower[i];\n",
    "        println!(\"{:7.2} | {:10.3} | {:8.3} | {:8.3} | {:13.3}\", \n",
    "                 x_test[i], predictions[i], lower[i], upper[i], width);\n",
    "    }\n",
    "    \n",
    "    // Training data vs predictions\n",
    "    println!(\"\\n📋 Training Data Fit:\");\n",
    "    let train_predictions = fitted.predict(&X_train);\n",
    "    for i in (0..n_train).step_by(6) {\n",
    "        println!(\"x = {:.2}: actual = {:.3}, predicted = {:.3}, residual = {:.3}\",\n",
    "                 x_train[i], y_train[i], train_predictions[i], \n",
    "                 y_train[i] - train_predictions[i]);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Models With and Without Intercept\n",
    "\n",
    "Sometimes forcing the regression through the origin is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Comparing Models: With vs Without Intercept\n",
      "===============================================\n",
      "\n",
      "True model: y = 2.50 × x (no intercept)\n",
      "\n",
      "Model WITH intercept:\n",
      "  y = -0.835 + 2.671 × x\n",
      "  R² = 0.8916\n",
      "\n",
      "Model WITHOUT intercept:\n",
      "  y = 2.547 × x\n",
      "  R² = 0.8890\n",
      "\n",
      "📈 Error Analysis:\n",
      "MSE with intercept:    7.528\n",
      "MSE without intercept: 7.708\n",
      "⚠️ Intercept model fits better (unexpected for this data)\n",
      "\n",
      "📋 Sample Predictions:\n",
      "X Value | True Y | With Intercept | Without Intercept | True Model\n",
      "--------|--------|----------------|-------------------|----------\n",
      "    0.0 |  -1.34 |          -0.84 |              0.00 |      0.00\n",
      "    2.0 |   5.66 |           4.62 |              5.20 |      5.10\n",
      "    4.1 |  10.65 |          10.07 |             10.40 |     10.20\n",
      "    6.1 |  13.55 |          15.52 |             15.60 |     15.31\n",
      "    8.2 |  21.57 |          20.97 |             20.79 |     20.41\n",
      "\n",
      "💡 Insight: When the true relationship passes through the origin,\n",
      "   forcing no intercept gives a better fit to the slope parameter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compare models with and without intercept\n",
    "{\n",
    "    // Generate proportional data (should pass through origin)\n",
    "    let n_samples = 50;\n",
    "    let mut rng = rand::thread_rng();\n",
    "    let noise_dist = Normal::new(0.0, 2.0).unwrap();\n",
    "    \n",
    "    let x = linspace(0.0, 10.0, n_samples);\n",
    "    let true_slope = 2.5;\n",
    "    \n",
    "    let mut y = VectorF64::zeros(n_samples);\n",
    "    for i in 0..n_samples {\n",
    "        let noise = noise_dist.sample(&mut rng);\n",
    "        y[i] = true_slope * x[i] + noise;  // No intercept in true model\n",
    "    }\n",
    "    \n",
    "    let X = ArrayF64::from_vector_column(&x);\n",
    "    \n",
    "    // Fit with intercept\n",
    "    let model_with = LinearRegression::new()\n",
    "        .with_intercept(true);\n",
    "    let fitted_with = model_with.fit(&X, &y).unwrap();\n",
    "    \n",
    "    // Fit without intercept\n",
    "    let model_without = LinearRegression::new()\n",
    "        .with_intercept(false);\n",
    "    let fitted_without = model_without.fit(&X, &y).unwrap();\n",
    "    \n",
    "    println!(\"📊 Comparing Models: With vs Without Intercept\");\n",
    "    println!(\"===============================================\\n\");\n",
    "    \n",
    "    println!(\"True model: y = {:.2} × x (no intercept)\\n\", true_slope);\n",
    "    \n",
    "    println!(\"Model WITH intercept:\");\n",
    "    println!(\"  y = {:.3} + {:.3} × x\", \n",
    "             fitted_with.intercept().unwrap(), \n",
    "             fitted_with.coefficients()[0]);\n",
    "    println!(\"  R² = {:.4}\", fitted_with.r_squared());\n",
    "    \n",
    "    println!(\"\\nModel WITHOUT intercept:\");\n",
    "    println!(\"  y = {:.3} × x\", fitted_without.coefficients()[0]);\n",
    "    println!(\"  R² = {:.4}\", fitted_without.r_squared());\n",
    "    \n",
    "    // Compare fit quality\n",
    "    let pred_with = fitted_with.predict(&X);\n",
    "    let pred_without = fitted_without.predict(&X);\n",
    "    \n",
    "    // Calculate mean squared errors\n",
    "    let mse_with = {\n",
    "        let residuals = &y - &pred_with;\n",
    "        let sq_residuals = &residuals * &residuals;\n",
    "        sq_residuals.sum_elements() / n_samples as f64\n",
    "    };\n",
    "    \n",
    "    let mse_without = {\n",
    "        let residuals = &y - &pred_without;\n",
    "        let sq_residuals = &residuals * &residuals;\n",
    "        sq_residuals.sum_elements() / n_samples as f64\n",
    "    };\n",
    "    \n",
    "    println!(\"\\n📈 Error Analysis:\");\n",
    "    println!(\"MSE with intercept:    {:.3}\", mse_with);\n",
    "    println!(\"MSE without intercept: {:.3}\", mse_without);\n",
    "    \n",
    "    if mse_without < mse_with {\n",
    "        println!(\"✅ No-intercept model fits better (as expected)\");\n",
    "    } else {\n",
    "        println!(\"⚠️ Intercept model fits better (unexpected for this data)\");\n",
    "    }\n",
    "    \n",
    "    // Show some sample predictions\n",
    "    println!(\"\\n📋 Sample Predictions:\");\n",
    "    println!(\"X Value | True Y | With Intercept | Without Intercept | True Model\");\n",
    "    println!(\"--------|--------|----------------|-------------------|----------\");\n",
    "    \n",
    "    for i in (0..n_samples).step_by(10) {\n",
    "        let true_y = true_slope * x[i];\n",
    "        println!(\"{:7.1} | {:6.2} | {:14.2} | {:17.2} | {:9.2}\",\n",
    "                 x[i], y[i], pred_with[i], pred_without[i], true_y);\n",
    "    }\n",
    "    \n",
    "    println!(\"\\n💡 Insight: When the true relationship passes through the origin,\");\n",
    "    println!(\"   forcing no intercept gives a better fit to the slope parameter.\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Normalization Effects\n",
    "\n",
    "Normalization can improve numerical stability and make coefficients comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "thread '<unnamed>' panicked at src/lib.rs:61:44:\n",
      "called `Result::unwrap()` on an `Err` value: LinearAlgebra(\"Matrix is singular and cannot be inverted\")\n",
      "stack backtrace:\n",
      "   0: __rustc::rust_begin_unwind\n",
      "             at /rustc/9982d6462bedf1e793f7b2dbd655a4e57cdf67d4/library/std/src/panicking.rs:697:5\n",
      "   1: core::panicking::panic_fmt\n",
      "             at /rustc/9982d6462bedf1e793f7b2dbd655a4e57cdf67d4/library/core/src/panicking.rs:75:14\n",
      "   2: core::result::unwrap_failed\n",
      "             at /rustc/9982d6462bedf1e793f7b2dbd655a4e57cdf67d4/library/core/src/result.rs:1761:5\n",
      "   3: std::panic::catch_unwind\n",
      "   4: run_user_code_7\n",
      "   5: evcxr::runtime::Runtime::run_loop\n",
      "   6: evcxr::runtime::runtime_hook\n",
      "   7: evcxr_jupyter::main\n",
      "note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\n"
     ]
    }
   ],
   "source": [
    "// Demonstrate normalization effects with features of different scales\n",
    "{\n",
    "    let n_samples = 100;\n",
    "    let mut rng = rand::thread_rng();\n",
    "    let noise_dist = Normal::new(0.0, 5.0).unwrap();\n",
    "    \n",
    "    // Features with very different scales\n",
    "    let age = linspace(20.0, 60.0, n_samples);           // Scale: tens\n",
    "    let income = linspace(30000.0, 150000.0, n_samples); // Scale: thousands\n",
    "    let score = linspace(0.0, 1.0, n_samples);           // Scale: fraction\n",
    "    \n",
    "    // Create feature matrix manually\n",
    "    let mut X = ArrayF64::zeros(n_samples, 3);\n",
    "    for i in 0..n_samples {\n",
    "        X[(i, 0)] = age[i];\n",
    "        X[(i, 1)] = income[i];\n",
    "        X[(i, 2)] = score[i];\n",
    "    }\n",
    "    \n",
    "    // Generate target with known relationships\n",
    "    let mut y = VectorF64::zeros(n_samples);\n",
    "    for i in 0..n_samples {\n",
    "        let noise = noise_dist.sample(&mut rng);\n",
    "        // All features have similar true importance\n",
    "        y[i] = 0.5 * age[i] + 0.0001 * income[i] + 50.0 * score[i] + noise;\n",
    "    }\n",
    "    \n",
    "    // Fit without normalization\n",
    "    let model_raw = LinearRegression::new()\n",
    "        .with_normalization(false);\n",
    "    let fitted_raw = model_raw.fit(&X, &y).unwrap();\n",
    "    \n",
    "    // Fit with normalization\n",
    "    let model_norm = LinearRegression::new()\n",
    "        .with_normalization(true);\n",
    "    let fitted_norm = model_norm.fit(&X, &y).unwrap();\n",
    "    \n",
    "    println!(\"📊 Feature Normalization Effects\");\n",
    "    println!(\"=================================\\n\");\n",
    "    \n",
    "    // Show feature scales\n",
    "    println!(\"Feature Scales:\");\n",
    "    println!(\"  Age range:    {:.0} - {:.0}\", age[0], age[n_samples-1]);\n",
    "    println!(\"  Income range: {:.0} - {:.0}\", income[0], income[n_samples-1]);\n",
    "    println!(\"  Score range:  {:.2} - {:.2}\\n\", score[0], score[n_samples-1]);\n",
    "    \n",
    "    println!(\"Raw coefficients (different scales):\");\n",
    "    let coef_raw = fitted_raw.coefficients();\n",
    "    println!(\"  Age (20-60):           {:.4}\", coef_raw[0]);\n",
    "    println!(\"  Income (30k-150k):     {:.6}\", coef_raw[1]);\n",
    "    println!(\"  Score (0-1):           {:.4}\\n\", coef_raw[2]);\n",
    "    \n",
    "    println!(\"Normalized coefficients (comparable):\");\n",
    "    let coef_norm = fitted_norm.coefficients();\n",
    "    println!(\"  Age:    {:.4}\", coef_norm[0]);\n",
    "    println!(\"  Income: {:.4}\", coef_norm[1]);\n",
    "    println!(\"  Score:  {:.4}\\n\", coef_norm[2]);\n",
    "    \n",
    "    println!(\"📈 Model Performance:\");\n",
    "    println!(\"R² (raw):        {:.4}\", fitted_raw.r_squared());\n",
    "    println!(\"R² (normalized): {:.4}\\n\", fitted_norm.r_squared());\n",
    "    \n",
    "    // Analyze feature importance\n",
    "    println!(\"📊 Feature Importance Analysis:\");\n",
    "    let features = vec![\"Age\", \"Income\", \"Score\"];\n",
    "    let raw_abs: Vec<f64> = coef_raw.iter().map(|&c| c.abs()).collect();\n",
    "    let norm_abs: Vec<f64> = coef_norm.iter().map(|&c| c.abs()).collect();\n",
    "    \n",
    "    println!(\"\\nRaw coefficient magnitudes (hard to compare due to scale):\");\n",
    "    for (i, feature) in features.iter().enumerate() {\n",
    "        println!(\"  {}: {:.6}\", feature, raw_abs[i]);\n",
    "    }\n",
    "    \n",
    "    println!(\"\\nNormalized coefficient magnitudes (directly comparable):\");\n",
    "    for (i, feature) in features.iter().enumerate() {\n",
    "        println!(\"  {}: {:.4}\", feature, norm_abs[i]);\n",
    "    }\n",
    "    \n",
    "    // Sort by importance (normalized)\n",
    "    let mut importance: Vec<(String, f64)> = features.iter()\n",
    "        .zip(norm_abs.iter())\n",
    "        .map(|(name, &coef)| (name.to_string(), coef))\n",
    "        .collect();\n",
    "    \n",
    "    importance.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());\n",
    "    \n",
    "    println!(\"\\n🏆 Feature Importance Ranking (normalized model):\");\n",
    "    for (rank, (name, imp)) in importance.iter().enumerate() {\n",
    "        println!(\"  {}. {}: {:.4}\", rank + 1, name, imp);\n",
    "    }\n",
    "    \n",
    "    println!(\"\\n💡 Insight: Normalization makes coefficients directly comparable\");\n",
    "    println!(\"   for feature importance, regardless of original scales.\");\n",
    "    println!(\"   Raw coefficients are misleading when features have different scales!\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- **Simple Linear Regression**: Single feature prediction with visualization\n",
    "- **Residual Analysis**: Checking model assumptions\n",
    "- **Multiple Regression**: Handling multiple features\n",
    "- **Statistical Inference**: P-values and confidence intervals\n",
    "- **Model Comparison**: With/without intercept\n",
    "- **Feature Normalization**: Effects on coefficient interpretation\n",
    "\n",
    "OLS regression is powerful for interpretable linear relationships, providing both predictions and statistical inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
