{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-title",
   "metadata": {},
   "source": [
    "# Broadcasting and Advanced Array Operations\n",
    "\n",
    "This notebook explores advanced array manipulation techniques in rustlab-math:\n",
    "- Broadcasting semantics and rules\n",
    "- Advanced indexing and slicing\n",
    "- Reshaping and dimension manipulation\n",
    "- Memory-efficient operations\n",
    "- Vectorized operations\n",
    "\n",
    "**Prerequisites**: Basic understanding of arrays and linear algebra\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Important**: The setup cell below follows Rust notebook best practices:\n",
    "- Dependencies and imports are declared at the **top level** (outside braces) so they persist across all cells\n",
    "- Test code is wrapped in braces `{}` to avoid persistence issues with complex types\n",
    "- This pattern ensures compatibility with both rust-analyzer and evcxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RustLab Broadcasting and Advanced Operations Demo\n",
      "Test array shape: 2x2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Setup Cell - dependencies and imports persist across all cells\n",
    ":dep rustlab-math = { path = \"..\" }\n",
    "\n",
    "// Top-level imports - these persist across all cells!\n",
    "use rustlab_math::*;\n",
    "use std::time::Instant;\n",
    "\n",
    "// Test setup in braces (variables don't persist, but confirms setup works)\n",
    "{\n",
    "    let test_array = array64![[1.0, 2.0], [3.0, 4.0]];\n",
    "    println!(\"✅ RustLab Broadcasting and Advanced Operations Demo\");\n",
    "    println!(\"Test array shape: {}x{}\", test_array.nrows(), test_array.ncols());\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadcasting-basics",
   "metadata": {},
   "source": [
    "## Broadcasting Fundamentals\n",
    "\n",
    "Broadcasting allows operations between arrays of different shapes by automatically expanding dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broadcast-examples",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix (3x3):\n",
      "   1.0    2.0    3.0 \n",
      "   4.0    5.0    6.0 \n",
      "   7.0    8.0    9.0 \n",
      "\n",
      "Matrix + 10 (scalar broadcast):\n",
      "  11.0   12.0   13.0 \n",
      "  14.0   15.0   16.0 \n",
      "  17.0   18.0   19.0 \n",
      "\n",
      "Matrix * 2 (scalar multiplication):\n",
      "   2.0    4.0    6.0 \n",
      "   8.0   10.0   12.0 \n",
      "  14.0   16.0   18.0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Basic broadcasting examples\n",
    "let matrix = array64![\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]\n",
    "];\n",
    "\n",
    "println!(\"Original matrix (3x3):\");\n",
    "for i in 0..matrix.nrows() {\n",
    "    for j in 0..matrix.ncols() {\n",
    "        print!(\"{:6.1} \", matrix.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Scalar broadcasting - adds 10 to every element\n",
    "let scalar_broadcast = &matrix + 10.0;\n",
    "println!(\"\\nMatrix + 10 (scalar broadcast):\");\n",
    "for i in 0..scalar_broadcast.nrows() {\n",
    "    for j in 0..scalar_broadcast.ncols() {\n",
    "        print!(\"{:6.1} \", scalar_broadcast.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Scalar multiplication\n",
    "let scaled_matrix = &matrix * 2.0;\n",
    "println!(\"\\nMatrix * 2 (scalar multiplication):\");\n",
    "for i in 0..scaled_matrix.nrows() {\n",
    "    for j in 0..scaled_matrix.ncols() {\n",
    "        print!(\"{:6.1} \", scaled_matrix.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadcasting-rules",
   "metadata": {},
   "source": [
    "## Broadcasting Rules and Compatibility\n",
    "\n",
    "Understanding when broadcasting works and when it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "broadcast-rules",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting compatibility examples:\n",
      "\n",
      "(2,3) * π → (2,3): Scalar broadcasting always works\n",
      "(3,3) + (3,3) → (3,3): Same shape works\n",
      "Result shape: 3x3\n",
      "\n",
      "Base matrix for operations:\n",
      "  1.0   2.0   3.0   4.0 \n",
      "  5.0   6.0   7.0   8.0 \n",
      "\n",
      "Doubled (using map):\n",
      "  2.0   4.0   6.0   8.0 \n",
      " 10.0  12.0  14.0  16.0 \n",
      "\n",
      "Exp(x/4) values:\n",
      "  1.284   1.649   2.117   2.718 \n",
      "  3.490   4.482   5.755   7.389 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Broadcasting rules demonstration\n",
    "println!(\"Broadcasting compatibility examples:\");\n",
    "\n",
    "// Scalar with any shape - always works\n",
    "let any_shape = ArrayF64::zeros(2, 3);\n",
    "let scalar_result = &any_shape * PI;\n",
    "println!(\"\\n(2,3) * π → (2,3): Scalar broadcasting always works\");\n",
    "\n",
    "// Same shape arrays - works perfectly\n",
    "let a1 = ArrayF64::ones(3, 3);\n",
    "let a2 = ArrayF64::ones(3, 3);\n",
    "let same_shape = &a1 + &a2;\n",
    "println!(\"(3,3) + (3,3) → (3,3): Same shape works\");\n",
    "println!(\"Result shape: {}x{}\", same_shape.nrows(), same_shape.ncols());\n",
    "\n",
    "// Element-wise operations\n",
    "let base = array64![\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [5.0, 6.0, 7.0, 8.0]\n",
    "];\n",
    "\n",
    "println!(\"\\nBase matrix for operations:\");\n",
    "for i in 0..base.nrows() {\n",
    "    for j in 0..base.ncols() {\n",
    "        print!(\"{:5.1} \", base.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Apply transformations using map()\n",
    "let doubled = base.map(|x| x * 2.0);\n",
    "println!(\"\\nDoubled (using map):\");\n",
    "for i in 0..doubled.nrows() {\n",
    "    for j in 0..doubled.ncols() {\n",
    "        print!(\"{:5.1} \", doubled.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Apply mathematical functions\n",
    "let exp_values = base.map(|x| (x / 4.0).exp());\n",
    "println!(\"\\nExp(x/4) values:\");\n",
    "for i in 0..exp_values.nrows() {\n",
    "    for j in 0..exp_values.ncols() {\n",
    "        print!(\"{:7.3} \", exp_values.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cwlx8s02g",
   "metadata": {},
   "source": [
    "## Array + Vector Broadcasting\n",
    "\n",
    "RustLab supports automatic broadcasting between arrays and vectors - a critical feature for data science and machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15wgsg65gvt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Array + Vector Broadcasting ===\n",
      "Original data matrix (3×4):\n",
      "  1.0   2.0   3.0   4.0 \n",
      "  5.0   6.0   7.0   8.0 \n",
      "  9.0  10.0  11.0  12.0 \n",
      "\n",
      "1. Row-wise Broadcasting (Most Common in ML)\n",
      "Feature means: [2.0, 4.0, 6.0, 8.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Array + Vector Broadcasting Examples\n",
    "println!(\"=== Array + Vector Broadcasting ===\");\n",
    "\n",
    "// Create sample data matrix (3 samples, 4 features)\n",
    "let data = array64![\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [5.0, 6.0, 7.0, 8.0],\n",
    "    [9.0, 10.0, 11.0, 12.0]\n",
    "];\n",
    "\n",
    "println!(\"Original data matrix (3×4):\");\n",
    "for i in 0..data.nrows() {\n",
    "    for j in 0..data.ncols() {\n",
    "        print!(\"{:5.1} \", data.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// 1. ROW-WISE BROADCASTING (Vector length = Matrix columns)\n",
    "println!(\"\\n1. Row-wise Broadcasting (Most Common in ML)\");\n",
    "let feature_means = vec64![2.0, 4.0, 6.0, 8.0];  // Length 4 = ncols\n",
    "println!(\"Feature means: {:?}\", feature_means.to_slice());\n",
    "\n",
    "// Automatic broadcasting: subtract mean from each feature (column)\n",
    "let centered_data = &data - &feature_means;\n",
    "println!(\"\\nCentered data (data - means):\");\n",
    "for i in 0..centered_data.nrows() {\n",
    "    for j in 0..centered_data.ncols() {\n",
    "        print!(\"{:6.1} \", centered_data.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Feature scaling example\n",
    "let feature_scales = vec64![1.0, 0.5, 2.0, 0.1];  // Length 4 = ncols\n",
    "let scaled_data = &centered_data * &feature_scales;\n",
    "println!(\"\\nScaled data (centered * scales):\");\n",
    "for i in 0..scaled_data.nrows() {\n",
    "    for j in 0..scaled_data.ncols() {\n",
    "        print!(\"{:6.2} \", scaled_data.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// 2. COLUMN-WISE BROADCASTING (Vector length = Matrix rows)\n",
    "println!(\"\\n2. Column-wise Broadcasting\");\n",
    "let sample_weights = vec64![1.0, 1.5, 0.8];  // Length 3 = nrows\n",
    "println!(\"Sample weights: {:?}\", sample_weights.to_slice());\n",
    "\n",
    "// Weight each sample (row) differently\n",
    "let weighted_data = &data * &sample_weights;\n",
    "println!(\"\\nWeighted data (each sample scaled):\");\n",
    "for i in 0..weighted_data.nrows() {\n",
    "    for j in 0..weighted_data.ncols() {\n",
    "        print!(\"{:6.1} \", weighted_data.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// 3. ALL ARITHMETIC OPERATIONS SUPPORT BROADCASTING\n",
    "println!(\"\\n3. All Arithmetic Operations\");\n",
    "\n",
    "// Addition (bias terms)\n",
    "let bias = vec64![10.0, 20.0, 30.0, 40.0];  // Feature-wise bias\n",
    "let biased = &data + &bias;\n",
    "println!(\"Addition (data + bias):\");\n",
    "for i in 0..biased.nrows().min(2) {\n",
    "    for j in 0..biased.ncols() {\n",
    "        print!(\"{:5.1} \", biased.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Division (normalization)\n",
    "let normalizers = vec64![2.0, 3.0, 4.0, 6.0];\n",
    "let normalized = &data / &normalizers;\n",
    "println!(\"\\nDivision (data / normalizers):\");\n",
    "for i in 0..normalized.nrows().min(2) {\n",
    "    for j in 0..normalized.ncols() {\n",
    "        print!(\"{:5.2} \", normalized.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// 4. COMMUTATIVE OPERATIONS\n",
    "println!(\"\\n4. Commutative Operations Work Both Ways\");\n",
    "let result1 = &data + &bias;\n",
    "let result2 = &bias + &data;\n",
    "println!(\"data + bias == bias + data? {}\", \n",
    "         result1.get(0, 0) == result2.get(0, 0));\n",
    "\n",
    "// 5. REAL-WORLD: FEATURE STANDARDIZATION\n",
    "println!(\"\\n5. Real-World Example: Z-Score Standardization\");\n",
    "\n",
    "// Calculate column means and stds\n",
    "let mut col_means = Vec::new();\n",
    "let mut col_stds = Vec::new();\n",
    "\n",
    "for j in 0..data.ncols() {\n",
    "    let mut sum = 0.0;\n",
    "    for i in 0..data.nrows() {\n",
    "        sum += data.get(i, j).unwrap();\n",
    "    }\n",
    "    let mean = sum / data.nrows() as f64;\n",
    "    col_means.push(mean);\n",
    "    \n",
    "    let mut sum_sq_diff = 0.0;\n",
    "    for i in 0..data.nrows() {\n",
    "        let diff = data.get(i, j).unwrap() - mean;\n",
    "        sum_sq_diff += diff * diff;\n",
    "    }\n",
    "    let std_dev = (sum_sq_diff / (data.nrows() - 1) as f64).sqrt();\n",
    "    col_stds.push(std_dev);\n",
    "}\n",
    "\n",
    "let means_vec = VectorF64::from_slice(&col_means);\n",
    "let stds_vec = VectorF64::from_slice(&col_stds);\n",
    "\n",
    "println!(\"Column means: {:?}\", col_means);\n",
    "println!(\"Column stds: {:?}\", col_stds);\n",
    "\n",
    "// Z-score standardization using broadcasting\n",
    "let z_scored = (&data - &means_vec) / &stds_vec;\n",
    "println!(\"\\nZ-score standardized data:\");\n",
    "for i in 0..z_scored.nrows() {\n",
    "    for j in 0..z_scored.ncols() {\n",
    "        print!(\"{:7.3} \", z_scored.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Verify: z-scored data should have ~0 mean and ~1 std\n",
    "let mut verify_sum = 0.0;\n",
    "for i in 0..z_scored.nrows() {\n",
    "    verify_sum += z_scored.get(i, 0).unwrap();\n",
    "}\n",
    "let verify_mean = verify_sum / z_scored.nrows() as f64;\n",
    "println!(\"\\nVerification - First column mean after z-score: {:.6}\", verify_mean);\n",
    "\n",
    "println!(\"\\n✨ Key Broadcasting Benefits:\");\n",
    "println!(\"• Natural mathematical syntax: data - means\");\n",
    "println!(\"• Automatic dimension detection (row-wise vs column-wise)\");\n",
    "println!(\"• All arithmetic operations supported (+, -, *, /)\");\n",
    "println!(\"• Commutative operations work both ways\");\n",
    "println!(\"• Essential for ML: feature normalization, batch processing\");\n",
    "println!(\"• Zero-cost: optimized implementations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8387xnx7",
   "metadata": {},
   "source": [
    "## Broadcasting vs Matrix Multiplication\n",
    "\n",
    "**Critical distinction**: Element-wise broadcasting (`*`) vs matrix multiplication (`^`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "l4v7fyp2tt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centered data (data - means):\n",
      "  -1.0   -2.0   -3.0   -4.0 \n",
      "   3.0    2.0    1.0    0.0 \n",
      "   7.0    6.0    5.0    4.0 \n",
      "\n",
      "Scaled data (centered * scales):\n",
      " -1.00  -1.00  -6.00  -0.40 \n",
      "  3.00   1.00   2.00   0.00 \n",
      "  7.00   3.00  10.00   0.40 \n",
      "\n",
      "2. Column-wise Broadcasting\n",
      "Sample weights: [1.0, 1.5, 0.8]\n",
      "\n",
      "Weighted data (each sample scaled):\n",
      "   1.0    2.0    3.0    4.0 \n",
      "   7.5    9.0   10.5   12.0 \n",
      "   7.2    8.0    8.8    9.6 \n",
      "\n",
      "3. All Arithmetic Operations\n",
      "Addition (data + bias):\n",
      " 11.0  22.0  33.0  44.0 \n",
      " 15.0  26.0  37.0  48.0 \n",
      "\n",
      "Division (data / normalizers):\n",
      " 0.50  0.67  0.75  0.67 \n",
      " 2.50  2.00  1.75  1.33 \n",
      "\n",
      "4. Commutative Operations Work Both Ways\n",
      "data + bias == bias + data? true\n",
      "\n",
      "5. Real-World Example: Z-Score Standardization\n",
      "Column means: [5.0, 6.0, 7.0, 8.0]\n",
      "Column stds: [4.0, 4.0, 4.0, 4.0]\n",
      "\n",
      "Z-score standardized data:\n",
      " -1.000  -1.000  -1.000  -1.000 \n",
      "  0.000   0.000   0.000   0.000 \n",
      "  1.000   1.000   1.000   1.000 \n",
      "\n",
      "Verification - First column mean after z-score: 0.000000\n",
      "\n",
      "✨ Key Broadcasting Benefits:\n",
      "• Natural mathematical syntax: data - means\n",
      "• Automatic dimension detection (row-wise vs column-wise)\n",
      "• All arithmetic operations supported (+, -, *, /)\n",
      "• Commutative operations work both ways\n",
      "• Essential for ML: feature normalization, batch processing\n",
      "• Zero-cost: optimized implementations\n",
      "=== Broadcasting vs Matrix Multiplication ===\n",
      "Data matrix (2×3):\n",
      "  1.0   2.0   3.0 \n",
      "  4.0   5.0   6.0 \n",
      "Weights vector: [0.1, 0.2, 0.3]\n",
      "\n",
      "1. Element-wise Broadcasting (data * weights):\n",
      "   Uses * operator → produces MATRIX output (same shape as input)\n",
      "   Result shape: 2×3\n",
      "   0.10    0.40    0.90 \n",
      "   0.40    1.00    1.80 \n",
      "   Interpretation: Each feature scaled by its weight\n",
      "\n",
      "2. Matrix Multiplication (data ^ weights):\n",
      "   Uses ^ operator → produces VECTOR output\n",
      "   Result shape: vector of length 2\n",
      "   1.40    3.20 \n",
      "   Interpretation: Weighted sum of features per sample\n",
      "\n",
      "3. When to Use Which Operation:\n",
      "\n",
      "   ✅ Use BROADCASTING (*) for:\n",
      "     • Feature scaling: scale each feature by different factor\n",
      "     • Normalization: divide by standard deviations\n"
     ]
    }
   ],
   "source": [
    "// Broadcasting vs Matrix Multiplication - Critical Distinction!\n",
    "println!(\"=== Broadcasting vs Matrix Multiplication ===\");\n",
    "\n",
    "let data = array64![\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0]\n",
    "];  // 2×3 matrix\n",
    "\n",
    "let weights = vec64![0.1, 0.2, 0.3];  // 3-element vector\n",
    "\n",
    "println!(\"Data matrix (2×3):\");\n",
    "for i in 0..data.nrows() {\n",
    "    for j in 0..data.ncols() {\n",
    "        print!(\"{:5.1} \", data.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "println!(\"Weights vector: {:?}\", weights.to_slice());\n",
    "\n",
    "// 1. ELEMENT-WISE BROADCASTING (using *)\n",
    "println!(\"\\n1. Element-wise Broadcasting (data * weights):\");\n",
    "println!(\"   Uses * operator → produces MATRIX output (same shape as input)\");\n",
    "let element_wise = &data * &weights;  // Broadcasting: each row scaled by weights\n",
    "println!(\"   Result shape: {}×{}\", element_wise.nrows(), element_wise.ncols());\n",
    "for i in 0..element_wise.nrows() {\n",
    "    for j in 0..element_wise.ncols() {\n",
    "        print!(\"{:7.2} \", element_wise.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "println!(\"   Interpretation: Each feature scaled by its weight\");\n",
    "\n",
    "// 2. MATRIX MULTIPLICATION (using ^)\n",
    "println!(\"\\n2. Matrix Multiplication (data ^ weights):\");\n",
    "println!(\"   Uses ^ operator → produces VECTOR output\");\n",
    "let matrix_mult = &data ^ &weights;    // Linear combination: weighted sum per row\n",
    "println!(\"   Result shape: vector of length {}\", matrix_mult.len());\n",
    "for i in 0..matrix_mult.len() {\n",
    "    print!(\"{:7.2} \", matrix_mult.get(i).unwrap());\n",
    "}\n",
    "println!();\n",
    "println!(\"   Interpretation: Weighted sum of features per sample\");\n",
    "\n",
    "// 3. WHEN TO USE WHICH\n",
    "println!(\"\\n3. When to Use Which Operation:\");\n",
    "\n",
    "println!(\"\\n   ✅ Use BROADCASTING (*) for:\");\n",
    "println!(\"     • Feature scaling: scale each feature by different factor\");\n",
    "println!(\"     • Normalization: divide by standard deviations\");\n",
    "println!(\"     • Element-wise operations: mask, multiply by probabilities\");\n",
    "println!(\"     • Output: Same shape as input matrix\");\n",
    "\n",
    "println!(\"\\n   ✅ Use MATRIX MULTIPLICATION (^) for:\");\n",
    "println!(\"     • Linear transformations: neural network layers\");\n",
    "println!(\"     • Weighted combinations: feature importance scoring\");  \n",
    "println!(\"     • Dot products: similarity calculations\");\n",
    "println!(\"     • Output: Reduced dimensions (matrix→vector, matrix→matrix)\");\n",
    "\n",
    "// 4. PRACTICAL EXAMPLES\n",
    "println!(\"\\n4. Practical Examples:\");\n",
    "\n",
    "// Feature normalization (broadcasting)\n",
    "let feature_stds = vec64![1.0, 2.0, 3.0];\n",
    "let normalized = &data / &feature_stds;\n",
    "println!(\"\\n   Feature normalization (broadcasting):\");\n",
    "println!(\"   data / stds = element-wise division\");\n",
    "for i in 0..normalized.nrows() {\n",
    "    for j in 0..normalized.ncols() {\n",
    "        print!(\"{:7.2} \", normalized.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Linear prediction (matrix multiplication)\n",
    "let linear_weights = vec64![0.5, 0.3, 0.2];  // Feature importance weights\n",
    "let predictions = &data ^ &linear_weights;\n",
    "println!(\"\\n   Linear prediction (matrix multiplication):\");\n",
    "println!(\"   data ^ weights = weighted sum per sample\");\n",
    "println!(\"   Predictions: {:?}\", predictions.to_slice());\n",
    "\n",
    "// Calculate manually to verify\n",
    "let manual_pred_0 = 1.0*0.5 + 2.0*0.3 + 3.0*0.2;  // First row\n",
    "let manual_pred_1 = 4.0*0.5 + 5.0*0.3 + 6.0*0.2;  // Second row\n",
    "println!(\"   Manual verification:\");\n",
    "println!(\"     Sample 0: 1*0.5 + 2*0.3 + 3*0.2 = {:.2}\", manual_pred_0);\n",
    "println!(\"     Sample 1: 4*0.5 + 5*0.3 + 6*0.2 = {:.2}\", manual_pred_1);\n",
    "\n",
    "// 5. COMMON MISTAKES TO AVOID\n",
    "println!(\"\\n5. ⚠️  Common Mistakes to Avoid:\");\n",
    "println!(\"   ❌ Using * when you want linear combinations (use ^ instead)\");\n",
    "println!(\"   ❌ Using ^ when you want element-wise scaling (use * instead)\");\n",
    "println!(\"   ❌ Expecting vector output from broadcasting (always produces matrix)\");\n",
    "println!(\"   ❌ Expecting matrix output from matrix mult (depends on dimensions)\");\n",
    "\n",
    "println!(\"\\n📝 Summary:\");\n",
    "println!(\"   • * = Element-wise broadcasting (Hadamard product)\");\n",
    "println!(\"   • ^ = Matrix multiplication (linear algebra)\");\n",
    "println!(\"   • Broadcasting preserves input matrix shape\");\n",
    "println!(\"   • Matrix multiplication reduces/transforms dimensions\");\n",
    "println!(\"   • Both are essential for different ML operations!\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-indexing",
   "metadata": {},
   "source": [
    "## Advanced Indexing and Slicing\n",
    "\n",
    "Sophisticated ways to extract and manipulate array data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indexing-slicing",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     • Element-wise operations: mask, multiply by probabilities\n",
      "     • Output: Same shape as input matrix\n",
      "\n",
      "   ✅ Use MATRIX MULTIPLICATION (^) for:\n",
      "     • Linear transformations: neural network layers\n",
      "     • Weighted combinations: feature importance scoring\n",
      "     • Dot products: similarity calculations\n",
      "     • Output: Reduced dimensions (matrix→vector, matrix→matrix)\n",
      "\n",
      "4. Practical Examples:\n",
      "\n",
      "   Feature normalization (broadcasting):\n",
      "   data / stds = element-wise division\n",
      "   1.00    1.00    1.00 \n",
      "   4.00    2.50    2.00 \n",
      "\n",
      "   Linear prediction (matrix multiplication):\n",
      "   data ^ weights = weighted sum per sample\n",
      "   Predictions: [1.7000000000000002, 4.7]\n",
      "   Manual verification:\n",
      "     Sample 0: 1*0.5 + 2*0.3 + 3*0.2 = 1.70\n",
      "     Sample 1: 4*0.5 + 5*0.3 + 6*0.2 = 4.70\n",
      "\n",
      "5. ⚠️  Common Mistakes to Avoid:\n",
      "   ❌ Using * when you want linear combinations (use ^ instead)\n",
      "   ❌ Using ^ when you want element-wise scaling (use * instead)\n",
      "   ❌ Expecting vector output from broadcasting (always produces matrix)\n",
      "   ❌ Expecting matrix output from matrix mult (depends on dimensions)\n",
      "\n",
      "📝 Summary:\n",
      "   • * = Element-wise broadcasting (Hadamard product)\n",
      "   • ^ = Matrix multiplication (linear algebra)\n",
      "   • Broadcasting preserves input matrix shape\n",
      "   • Matrix multiplication reduces/transforms dimensions\n",
      "   • Both are essential for different ML operations!\n",
      "Created test matrix (5x6) filled with zeros\n",
      "Element at [2,3]: 0\n",
      "\n",
      "Test data matrix (5x6):\n",
      "   1    2    3    4    5    6 \n"
     ]
    }
   ],
   "source": [
    "// Create test data for indexing examples\n",
    "let data = ArrayF64::zeros(5, 6);\n",
    "println!(\"Created test matrix (5x6) filled with zeros\");\n",
    "\n",
    "// Basic element access\n",
    "if let Some(val) = data.get(2, 3) {\n",
    "    println!(\"Element at [2,3]: {}\", val);\n",
    "}\n",
    "\n",
    "// Test with actual data\n",
    "let test_data = array64![\n",
    "    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0, 10.0, 11.0, 12.0],\n",
    "    [13.0, 14.0, 15.0, 16.0, 17.0, 18.0],\n",
    "    [19.0, 20.0, 21.0, 22.0, 23.0, 24.0],\n",
    "    [25.0, 26.0, 27.0, 28.0, 29.0, 30.0]\n",
    "];\n",
    "\n",
    "println!(\"\\nTest data matrix (5x6):\");\n",
    "for i in 0..test_data.nrows() {\n",
    "    for j in 0..test_data.ncols() {\n",
    "        print!(\"{:4.0} \", test_data.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Row access example\n",
    "println!(\"\\nRow 2 elements:\");\n",
    "for j in 0..test_data.ncols() {\n",
    "    print!(\"{:4.0} \", test_data.get(2, j).unwrap());\n",
    "}\n",
    "println!();\n",
    "\n",
    "// Column access example  \n",
    "println!(\"\\nColumn 3 elements:\");\n",
    "for i in 0..test_data.nrows() {\n",
    "    print!(\"{:4.0} \", test_data.get(i, 3).unwrap());\n",
    "}\n",
    "println!();\n",
    "\n",
    "// Corner elements\n",
    "println!(\"\\nCorner elements:\");\n",
    "println!(\"  Top-left [0,0]: {}\", test_data.get(0, 0).unwrap());\n",
    "println!(\"  Top-right [0,{}]: {}\", test_data.ncols()-1, test_data.get(0, test_data.ncols()-1).unwrap());\n",
    "println!(\"  Bottom-left [{},0]: {}\", test_data.nrows()-1, test_data.get(test_data.nrows()-1, 0).unwrap());\n",
    "println!(\"  Bottom-right [{},{}]: {}\", test_data.nrows()-1, test_data.ncols()-1, \n",
    "         test_data.get(test_data.nrows()-1, test_data.ncols()-1).unwrap());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reshaping-views",
   "metadata": {},
   "source": [
    "## Reshaping and Views\n",
    "\n",
    "Efficient array manipulation without copying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reshape-views",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7    8    9   10   11   12 \n",
      "  13   14   15   16   17   18 \n",
      "  19   20   21   22   23   24 \n",
      "  25   26   27   28   29   30 \n",
      "\n",
      "Row 2 elements:\n",
      "  13   14   15   16   17   18 \n",
      "\n",
      "Column 3 elements:\n",
      "   4   10   16   22   28 \n",
      "\n",
      "Corner elements:\n",
      "  Top-left [0,0]: 1\n",
      "  Top-right [0,5]: 6\n",
      "  Bottom-left [4,0]: 25\n",
      "  Bottom-right [4,5]: 30\n",
      "Original array (2x6):\n",
      "  1.0   2.0   3.0   4.0   5.0   6.0 \n",
      "  7.0   8.0   9.0  10.0  11.0  12.0 \n",
      "\n",
      "Transposed view (6x2) - no data copying:\n",
      "  1.0   7.0 \n",
      "  2.0   8.0 \n",
      "  3.0   9.0 \n",
      "  4.0  10.0 \n"
     ]
    }
   ],
   "source": [
    "// Original array for reshaping and views\n",
    "let original = array64![\n",
    "    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0, 10.0, 11.0, 12.0]\n",
    "];\n",
    "\n",
    "println!(\"Original array (2x6):\");\n",
    "for i in 0..original.nrows() {\n",
    "    for j in 0..original.ncols() {\n",
    "        print!(\"{:5.1} \", original.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Transpose (efficient view operation)\n",
    "let transposed = original.transpose();\n",
    "println!(\"\\nTransposed view (6x2) - no data copying:\");\n",
    "for i in 0..transposed.nrows().min(4) {  // Show first 4 rows\n",
    "    for j in 0..transposed.ncols() {\n",
    "        print!(\"{:5.1} \", transposed.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "println!(\"... (showing first 4 rows of 6)\");\n",
    "\n",
    "// Create different sized arrays to demonstrate shape flexibility\n",
    "let vector_like = array64![[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]];  // 1x8\n",
    "println!(\"\\nVector-like array (1x8):\");\n",
    "for j in 0..vector_like.ncols() {\n",
    "    print!(\"{:4.1} \", vector_like.get(0, j).unwrap());\n",
    "}\n",
    "println!();\n",
    "\n",
    "let column_like = array64![\n",
    "    [1.0], [2.0], [3.0], [4.0], [5.0], [6.0]\n",
    "];  // 6x1\n",
    "println!(\"\\nColumn-like array (6x1):\");\n",
    "for i in 0..column_like.nrows() {\n",
    "    println!(\"{:4.1}\", column_like.get(i, 0).unwrap());\n",
    "}\n",
    "\n",
    "// Demonstrate efficient transpose\n",
    "println!(\"\\nTranspose of column becomes row (1x6):\");\n",
    "let transposed_col = column_like.transpose();\n",
    "for j in 0..transposed_col.ncols() {\n",
    "    print!(\"{:4.1} \", transposed_col.get(0, j).unwrap());\n",
    "}\n",
    "println!();\n",
    "\n",
    "// Show that transpose is a view operation (very fast)\n",
    "use std::time::Instant;\n",
    "let large_test = ArrayF64::ones(100, 200);\n",
    "let start = Instant::now();\n",
    "let _transposed_large = large_test.transpose();\n",
    "let transpose_time = start.elapsed();\n",
    "println!(\"\\nTransposing 100x200 matrix: {:?} (metadata-only operation)\", transpose_time);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-efficient",
   "metadata": {},
   "source": [
    "## Memory-Efficient Operations\n",
    "\n",
    "Techniques to minimize memory usage and maximize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "memory-efficient-ops",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... (showing first 4 rows of 6)\n",
      "\n",
      "Vector-like array (1x8):\n",
      " 1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0 \n",
      "\n",
      "Column-like array (6x1):\n",
      " 1.0\n",
      " 2.0\n",
      " 3.0\n",
      " 4.0\n",
      " 5.0\n",
      " 6.0\n",
      "\n",
      "Transpose of column becomes row (1x6):\n",
      " 1.0  2.0  3.0  4.0  5.0  6.0 \n",
      "\n",
      "Transposing 100x200 matrix: 85.439µs (metadata-only operation)\n",
      "Memory efficiency examples with 100x100 arrays:\n",
      "Array size: 78.12 KB\n",
      "\n",
      "Operation timing comparisons:\n",
      "Scalar addition (array + 1.0): 90.278µs\n",
      "Scalar multiplication (array * 2.0): 92.732µs\n",
      "Array addition (array + array): 124.528µs\n",
      "Map operation (x + 1.0): 98.15µs\n",
      "Complex map ((x+1).exp().sin()): 405.381µs\n",
      "Transpose operation: 64.411µs (metadata-only)\n",
      "Manual sum via get(): 30.263µs (sum = 0)\n",
      "\n",
      "Memory efficiency insights:\n",
      "- Scalar operations: Very fast, optimized\n",
      "- Array operations: Efficient for same-size arrays\n",
      "- Map operations: Good for element-wise transforms\n",
      "- Transpose: Essentially free (metadata change)\n",
      "- Manual access: Slower but flexible\n"
     ]
    }
   ],
   "source": [
    "use std::time::Instant;\n",
    "\n",
    "// Create test arrays for memory efficiency demonstration\n",
    "let size = 100;  // Using moderate size for demonstration\n",
    "let large_array = ArrayF64::zeros(size, size);\n",
    "\n",
    "println!(\"Memory efficiency examples with {}x{} arrays:\", size, size);\n",
    "println!(\"Array size: {:.2} KB\", (size * size * 8) as f64 / 1024.0);\n",
    "\n",
    "// Compare different operation approaches\n",
    "println!(\"\\nOperation timing comparisons:\");\n",
    "\n",
    "// Scalar operations (very efficient)\n",
    "let start = Instant::now();\n",
    "let scalar_add_result = &large_array + 1.0;\n",
    "let scalar_add_time = start.elapsed();\n",
    "println!(\"Scalar addition (array + 1.0): {:?}\", scalar_add_time);\n",
    "\n",
    "let start = Instant::now();\n",
    "let scalar_mult_result = &large_array * 2.0;\n",
    "let scalar_mult_time = start.elapsed();\n",
    "println!(\"Scalar multiplication (array * 2.0): {:?}\", scalar_mult_time);\n",
    "\n",
    "// Array-array operations (same size)\n",
    "let another_array = ArrayF64::ones(size, size);\n",
    "let start = Instant::now();\n",
    "let array_add_result = &large_array + &another_array;\n",
    "let array_add_time = start.elapsed();\n",
    "println!(\"Array addition (array + array): {:?}\", array_add_time);\n",
    "\n",
    "// Element-wise map operations\n",
    "let start = Instant::now();\n",
    "let map_result = large_array.map(|x| x + 1.0);\n",
    "let map_time = start.elapsed();\n",
    "println!(\"Map operation (x + 1.0): {:?}\", map_time);\n",
    "\n",
    "// More complex map operation\n",
    "let start = Instant::now();\n",
    "let complex_map = large_array.map(|x| (x + 1.0).exp().sin());\n",
    "let complex_map_time = start.elapsed();\n",
    "println!(\"Complex map ((x+1).exp().sin()): {:?}\", complex_map_time);\n",
    "\n",
    "// Transpose operation (metadata only)\n",
    "let start = Instant::now();\n",
    "let transpose_result = large_array.transpose();\n",
    "let transpose_time = start.elapsed();\n",
    "println!(\"Transpose operation: {:?} (metadata-only)\", transpose_time);\n",
    "\n",
    "// Manual element access for comparison\n",
    "let start = Instant::now();\n",
    "let mut manual_sum = 0.0;\n",
    "for i in 0..size {\n",
    "    for j in 0..size {\n",
    "        manual_sum += large_array.get(i, j).unwrap();\n",
    "    }\n",
    "}\n",
    "let manual_time = start.elapsed();\n",
    "println!(\"Manual sum via get(): {:?} (sum = {})\", manual_time, manual_sum);\n",
    "\n",
    "println!(\"\\nMemory efficiency insights:\");\n",
    "println!(\"- Scalar operations: Very fast, optimized\");\n",
    "println!(\"- Array operations: Efficient for same-size arrays\");\n",
    "println!(\"- Map operations: Good for element-wise transforms\");\n",
    "println!(\"- Transpose: Essentially free (metadata change)\");\n",
    "println!(\"- Manual access: Slower but flexible\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorized-ops",
   "metadata": {},
   "source": [
    "## Vectorized Operations\n",
    "\n",
    "Efficient batch operations that leverage SIMD instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vectorized-operations",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized operations demonstration:\n",
      "Test array (3x4):\n",
      "   1    2    3    4 \n",
      "   5    6    7    8 \n",
      "   9   10   11   12 \n",
      "\n",
      "Vectorized mathematical functions:\n",
      "Sin values (timing: 1.63µs):\n",
      "  0.841   0.909   0.141  -0.757 \n",
      " -0.959  -0.279   0.657   0.989 \n",
      "  0.412  -0.544  -1.000  -0.537 \n",
      "\n",
      "Exp(x/10) values (timing: 1.223µs):\n",
      "  1.105   1.221   1.350   1.492 \n",
      "  1.649   1.822   2.014   2.226 \n",
      "  2.460   2.718   3.004   3.320 \n",
      "\n",
      "Complex expression sqrt(sin²(x) + cos²(x)) (timing: 1.222µs):\n",
      " 1.0000  1.0000  1.0000  1.0000 \n",
      " 1.0000  1.0000  1.0000  1.0000 \n",
      " 1.0000  1.0000  1.0000  1.0000 \n",
      "\n",
      "Statistics operations:\n",
      "  Sum: 78.00\n",
      "  Mean: 6.50\n",
      "  Min: 1.00\n",
      "  Max: 12.00\n",
      "  Count: 12\n",
      "\n",
      "Performance comparison on 1000x1000 array:\n",
      "Vectorized sin operation: 17.482954ms\n",
      "Manual element-by-element: 49.041736ms\n",
      "Speedup: 2.8x faster with vectorized approach\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use std::time::Instant;\n",
    "\n",
    "// Create test data for vectorized operations\n",
    "let test_array = array64![\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [5.0, 6.0, 7.0, 8.0],\n",
    "    [9.0, 10.0, 11.0, 12.0]\n",
    "];\n",
    "\n",
    "println!(\"Vectorized operations demonstration:\");\n",
    "println!(\"Test array (3x4):\");\n",
    "for i in 0..test_array.nrows() {\n",
    "    for j in 0..test_array.ncols() {\n",
    "        print!(\"{:4.0} \", test_array.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Mathematical functions using map (vectorized)\n",
    "println!(\"\\nVectorized mathematical functions:\");\n",
    "\n",
    "let start = Instant::now();\n",
    "let sin_values = test_array.map(|x| x.sin());\n",
    "let sin_time = start.elapsed();\n",
    "println!(\"Sin values (timing: {:?}):\", sin_time);\n",
    "for i in 0..sin_values.nrows() {\n",
    "    for j in 0..sin_values.ncols() {\n",
    "        print!(\"{:7.3} \", sin_values.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "let start = Instant::now();\n",
    "let exp_values = test_array.map(|x| (x / 10.0).exp());\n",
    "let exp_time = start.elapsed();\n",
    "println!(\"\\nExp(x/10) values (timing: {:?}):\", exp_time);\n",
    "for i in 0..exp_values.nrows() {\n",
    "    for j in 0..exp_values.ncols() {\n",
    "        print!(\"{:7.3} \", exp_values.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Complex vectorized expressions\n",
    "let start = Instant::now();\n",
    "let complex_expr = test_array.map(|x| (x.sin().powi(2) + x.cos().powi(2)).sqrt());\n",
    "let complex_time = start.elapsed();\n",
    "println!(\"\\nComplex expression sqrt(sin²(x) + cos²(x)) (timing: {:?}):\", complex_time);\n",
    "for i in 0..complex_expr.nrows() {\n",
    "    for j in 0..complex_expr.ncols() {\n",
    "        print!(\"{:7.4} \", complex_expr.get(i, j).unwrap());  // Should be ~1.0\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Statistics operations (manual implementation)\n",
    "println!(\"\\nStatistics operations:\");\n",
    "\n",
    "// Calculate statistics manually for demonstration\n",
    "let mut sum_val = 0.0;\n",
    "let mut min_val = f64::MAX;\n",
    "let mut max_val = f64::MIN;\n",
    "let mut count = 0;\n",
    "\n",
    "for i in 0..test_array.nrows() {\n",
    "    for j in 0..test_array.ncols() {\n",
    "        let val = test_array.get(i, j).unwrap();\n",
    "        sum_val += val;\n",
    "        min_val = min_val.min(val);\n",
    "        max_val = max_val.max(val);\n",
    "        count += 1;\n",
    "    }\n",
    "}\n",
    "\n",
    "let mean_val = sum_val / count as f64;\n",
    "\n",
    "println!(\"  Sum: {:.2}\", sum_val);\n",
    "println!(\"  Mean: {:.2}\", mean_val);\n",
    "println!(\"  Min: {:.2}\", min_val);\n",
    "println!(\"  Max: {:.2}\", max_val);\n",
    "println!(\"  Count: {}\", count);\n",
    "\n",
    "// Demonstrate vectorized vs scalar comparison on larger array\n",
    "let large_size = 1000;\n",
    "let large_test = ArrayF64::ones(large_size, large_size);\n",
    "\n",
    "println!(\"\\nPerformance comparison on {}x{} array:\", large_size, large_size);\n",
    "\n",
    "// Vectorized operation using map\n",
    "let start = Instant::now();\n",
    "let _vectorized_result = large_test.map(|x| x.sin());\n",
    "let vectorized_time = start.elapsed();\n",
    "println!(\"Vectorized sin operation: {:?}\", vectorized_time);\n",
    "\n",
    "// Manual element-by-element (for comparison)\n",
    "let start = Instant::now();\n",
    "let mut manual_results = Vec::with_capacity(large_size * large_size);\n",
    "for i in 0..large_size {\n",
    "    for j in 0..large_size {\n",
    "        manual_results.push(large_test.get(i, j).unwrap().sin());\n",
    "    }\n",
    "}\n",
    "let manual_time = start.elapsed();\n",
    "println!(\"Manual element-by-element: {:?}\", manual_time);\n",
    "\n",
    "if manual_time.as_nanos() > 0 && vectorized_time.as_nanos() > 0 {\n",
    "    let speedup = manual_time.as_nanos() as f64 / vectorized_time.as_nanos() as f64;\n",
    "    println!(\"Speedup: {:.1}x faster with vectorized approach\", speedup);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-broadcasting",
   "metadata": {},
   "source": [
    "## Advanced Broadcasting Patterns\n",
    "\n",
    "Sophisticated broadcasting use cases for real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "advanced-broadcast-patterns",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Broadcasting Patterns:\n",
      "\n",
      "1. Matrix Operations and Broadcasting\n",
      "Matrix A:\n",
      "  1.0   2.0 \n",
      "  3.0   4.0 \n",
      "Matrix B:\n",
      "  5.0   6.0 \n",
      "  7.0   8.0 \n",
      "\n",
      "Element-wise multiplication (A ⊙ B):\n",
      "  5.0  12.0 \n",
      " 21.0  32.0 \n",
      "\n",
      "Matrix multiplication (A × B):\n",
      " 19.0  22.0 \n",
      " 43.0  50.0 \n",
      "\n",
      "2. Feature Standardization Example\n",
      "Original features (5 samples x 3 features):\n",
      "    1.0    10.0   100.0 \n",
      "    2.0    20.0   200.0 \n",
      "    3.0    30.0   300.0 \n",
      "    4.0    40.0   400.0 \n",
      "    5.0    50.0   500.0 \n",
      "\n",
      "Feature statistics:\n",
      "  Feature 0: mean=3.00, std=1.58\n",
      "  Feature 1: mean=30.00, std=15.81\n",
      "  Feature 2: mean=300.00, std=158.11\n",
      "\n",
      "Standardized features:\n",
      "  -1.265   -1.265   -1.265 \n",
      "  -0.632   -0.632   -0.632 \n",
      "   0.000    0.000    0.000 \n",
      "   0.632    0.632    0.632 \n",
      "   1.265    1.265    1.265 \n",
      "\n",
      "3. Softmax Function with Numerical Stability\n",
      "Logits:\n",
      "  2.0   1.0   0.1 \n",
      "  1.0   3.0   0.2 \n",
      "  0.5   2.0   1.5 \n",
      "\n",
      "Softmax probabilities (each row sums to 1):\n",
      " 0.6590  0.2424  0.0986   (sum: 1.0000)\n",
      " 0.1131  0.8360  0.0508   (sum: 1.0000)\n",
      " 0.1220  0.5465  0.3315   (sum: 1.0000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Advanced Broadcasting Patterns for Real-World Applications\n",
    "println!(\"Advanced Broadcasting Patterns:\");\n",
    "\n",
    "// Pattern 1: Matrix-Matrix Operations\n",
    "println!(\"\\n1. Matrix Operations and Broadcasting\");\n",
    "\n",
    "let a_mat = array64![[1.0, 2.0], [3.0, 4.0]];\n",
    "let b_mat = array64![[5.0, 6.0], [7.0, 8.0]];\n",
    "\n",
    "println!(\"Matrix A:\");\n",
    "for i in 0..a_mat.nrows() {\n",
    "    for j in 0..a_mat.ncols() {\n",
    "        print!(\"{:5.1} \", a_mat.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "println!(\"Matrix B:\");\n",
    "for i in 0..b_mat.nrows() {\n",
    "    for j in 0..b_mat.ncols() {\n",
    "        print!(\"{:5.1} \", b_mat.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Element-wise multiplication (Hadamard product)\n",
    "let hadamard = &a_mat * &b_mat;\n",
    "println!(\"\\nElement-wise multiplication (A ⊙ B):\");\n",
    "for i in 0..hadamard.nrows() {\n",
    "    for j in 0..hadamard.ncols() {\n",
    "        print!(\"{:5.1} \", hadamard.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Matrix multiplication \n",
    "let matmul = &a_mat ^ &b_mat;\n",
    "println!(\"\\nMatrix multiplication (A × B):\");\n",
    "for i in 0..matmul.nrows() {\n",
    "    for j in 0..matmul.ncols() {\n",
    "        print!(\"{:5.1} \", matmul.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Pattern 2: Feature Standardization (Z-score normalization)\n",
    "println!(\"\\n2. Feature Standardization Example\");\n",
    "let features = array64![\n",
    "    [1.0, 10.0, 100.0],\n",
    "    [2.0, 20.0, 200.0],\n",
    "    [3.0, 30.0, 300.0],\n",
    "    [4.0, 40.0, 400.0],\n",
    "    [5.0, 50.0, 500.0]\n",
    "];  // 5 samples, 3 features\n",
    "\n",
    "println!(\"Original features (5 samples x 3 features):\");\n",
    "for i in 0..features.nrows() {\n",
    "    for j in 0..features.ncols() {\n",
    "        print!(\"{:7.1} \", features.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Calculate mean for each feature (column)\n",
    "let mut col_means = Vec::new();\n",
    "let mut col_stds = Vec::new();\n",
    "\n",
    "for j in 0..features.ncols() {\n",
    "    let mut sum = 0.0;\n",
    "    for i in 0..features.nrows() {\n",
    "        sum += features.get(i, j).unwrap();\n",
    "    }\n",
    "    let mean = sum / features.nrows() as f64;\n",
    "    col_means.push(mean);\n",
    "    \n",
    "    // Calculate standard deviation\n",
    "    let mut sum_sq_diff = 0.0;\n",
    "    for i in 0..features.nrows() {\n",
    "        let diff = features.get(i, j).unwrap() - mean;\n",
    "        sum_sq_diff += diff * diff;\n",
    "    }\n",
    "    let std_dev = (sum_sq_diff / (features.nrows() - 1) as f64).sqrt();\n",
    "    col_stds.push(std_dev);\n",
    "}\n",
    "\n",
    "println!(\"\\nFeature statistics:\");\n",
    "for j in 0..features.ncols() {\n",
    "    println!(\"  Feature {}: mean={:.2}, std={:.2}\", j, col_means[j], col_stds[j]);\n",
    "}\n",
    "\n",
    "// Standardize: (x - μ) / σ using manual iteration\n",
    "let mut standardized_data = Vec::new();\n",
    "for i in 0..features.nrows() {\n",
    "    for j in 0..features.ncols() {\n",
    "        let val = features.get(i, j).unwrap();\n",
    "        let standardized_val = (val - col_means[j]) / col_stds[j];\n",
    "        standardized_data.push(standardized_val);\n",
    "    }\n",
    "}\n",
    "let standardized = ArrayF64::from_slice(&standardized_data, features.nrows(), features.ncols()).unwrap();\n",
    "\n",
    "println!(\"\\nStandardized features:\");\n",
    "for i in 0..standardized.nrows() {\n",
    "    for j in 0..standardized.ncols() {\n",
    "        print!(\"{:8.3} \", standardized.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Pattern 3: Softmax Function\n",
    "println!(\"\\n3. Softmax Function with Numerical Stability\");\n",
    "let logits = array64![\n",
    "    [2.0, 1.0, 0.1],\n",
    "    [1.0, 3.0, 0.2],\n",
    "    [0.5, 2.0, 1.5]\n",
    "];\n",
    "\n",
    "println!(\"Logits:\");\n",
    "for i in 0..logits.nrows() {\n",
    "    for j in 0..logits.ncols() {\n",
    "        print!(\"{:5.1} \", logits.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Numerically stable softmax: subtract max for each row\n",
    "let mut stable_softmax = Vec::new();\n",
    "for i in 0..logits.nrows() {\n",
    "    // Find max in row\n",
    "    let mut row_max = f64::NEG_INFINITY;\n",
    "    for j in 0..logits.ncols() {\n",
    "        row_max = row_max.max(logits.get(i, j).unwrap());\n",
    "    }\n",
    "    \n",
    "    // Compute exp(x - max) for numerical stability\n",
    "    let mut row_exp_vals = Vec::new();\n",
    "    let mut sum_exp = 0.0;\n",
    "    for j in 0..logits.ncols() {\n",
    "        let stable_val = (logits.get(i, j).unwrap() - row_max).exp();\n",
    "        row_exp_vals.push(stable_val);\n",
    "        sum_exp += stable_val;\n",
    "    }\n",
    "    \n",
    "    // Normalize to get probabilities\n",
    "    let row_probs: Vec<f64> = row_exp_vals.iter().map(|&x| x / sum_exp).collect();\n",
    "    stable_softmax.push(row_probs);\n",
    "}\n",
    "\n",
    "println!(\"\\nSoftmax probabilities (each row sums to 1):\");\n",
    "for (i, row) in stable_softmax.iter().enumerate() {\n",
    "    for (j, &prob) in row.iter().enumerate() {\n",
    "        print!(\"{:7.4} \", prob);\n",
    "    }\n",
    "    let row_sum: f64 = row.iter().sum();\n",
    "    println!(\"  (sum: {:.4})\", row_sum);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimension-manipulation",
   "metadata": {},
   "source": [
    "## Dimension Manipulation and Tensor Operations\n",
    "\n",
    "Advanced techniques for working with multi-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tensor-operations",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension Manipulation:\n",
      "Original vector: [1.0, 2.0, 3.0, 4.0]\n",
      "\n",
      "As row matrix (1x4):\n",
      " 1.0  2.0  3.0  4.0 \n",
      "\n",
      "As column matrix (4x1):\n",
      " 1.0\n",
      " 2.0\n",
      " 3.0\n",
      " 4.0\n",
      "\n",
      "Outer product concept (4x4 result):\n",
      "  1.0   2.0   3.0   4.0 \n",
      "  2.0   4.0   6.0   8.0 \n",
      "  3.0   6.0   9.0  12.0 \n",
      "  4.0   8.0  12.0  16.0 \n",
      "\n",
      "Matrix concatenation concepts:\n",
      "Matrix A (2x2):\n",
      "  1.0   2.0 \n",
      "  3.0   4.0 \n",
      "Matrix B (2x2):\n",
      "  5.0   6.0 \n",
      "  7.0   8.0 \n",
      "\n",
      "Vertical stacking concept (A above B):\n",
      "  1.0   2.0 \n",
      "  3.0   4.0 \n",
      "  5.0   6.0 \n",
      "  7.0   8.0 \n",
      "\n",
      "Horizontal stacking concept (A beside B):\n",
      "  1.0   2.0      5.0   6.0 \n",
      "  3.0   4.0      7.0   8.0 \n",
      "\n",
      "Array shape flexibility:\n",
      "  1x8 (row vector) -> 1x8 = 8 elements\n",
      "  8x1 (column vector) -> 8x1 = 8 elements\n",
      "  2x4 (wide matrix) -> 2x4 = 8 elements\n",
      "  4x2 (tall matrix) -> 4x2 = 8 elements\n",
      "  3x3 (square matrix) -> 3x3 = 9 elements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Dimension Manipulation Examples\n",
    "println!(\"Dimension Manipulation:\");\n",
    "\n",
    "let vector_data = vec64![1.0, 2.0, 3.0, 4.0];\n",
    "println!(\"Original vector: {:?}\", vector_data.to_slice());\n",
    "\n",
    "// Create different shaped arrays\n",
    "let row_matrix = array64![[1.0, 2.0, 3.0, 4.0]];      // 1x4\n",
    "let col_matrix = array64![[1.0], [2.0], [3.0], [4.0]]; // 4x1\n",
    "\n",
    "println!(\"\\nAs row matrix (1x4):\");\n",
    "for j in 0..row_matrix.ncols() {\n",
    "    print!(\"{:4.1} \", row_matrix.get(0, j).unwrap());\n",
    "}\n",
    "println!();\n",
    "\n",
    "println!(\"\\nAs column matrix (4x1):\");\n",
    "for i in 0..col_matrix.nrows() {\n",
    "    println!(\"{:4.1}\", col_matrix.get(i, 0).unwrap());\n",
    "}\n",
    "\n",
    "// Demonstrate outer product concept manually\n",
    "let outer_result = ArrayF64::zeros(4, 4);\n",
    "println!(\"\\nOuter product concept (4x4 result):\");\n",
    "for i in 0..col_matrix.nrows() {\n",
    "    for j in 0..row_matrix.ncols() {\n",
    "        let val = col_matrix.get(i, 0).unwrap() * row_matrix.get(0, j).unwrap();\n",
    "        print!(\"{:5.1} \", val);\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Matrix operations examples\n",
    "println!(\"\\nMatrix concatenation concepts:\");\n",
    "let A = array64![[1.0, 2.0], [3.0, 4.0]];\n",
    "let B = array64![[5.0, 6.0], [7.0, 8.0]];\n",
    "\n",
    "println!(\"Matrix A (2x2):\");\n",
    "for i in 0..A.nrows() {\n",
    "    for j in 0..A.ncols() {\n",
    "        print!(\"{:5.1} \", A.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "println!(\"Matrix B (2x2):\");\n",
    "for i in 0..B.nrows() {\n",
    "    for j in 0..B.ncols() {\n",
    "        print!(\"{:5.1} \", B.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Simulate vertical stacking manually\n",
    "println!(\"\\nVertical stacking concept (A above B):\");\n",
    "for i in 0..A.nrows() {\n",
    "    for j in 0..A.ncols() {\n",
    "        print!(\"{:5.1} \", A.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "for i in 0..B.nrows() {\n",
    "    for j in 0..B.ncols() {\n",
    "        print!(\"{:5.1} \", B.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Simulate horizontal stacking manually\n",
    "println!(\"\\nHorizontal stacking concept (A beside B):\");\n",
    "for i in 0..A.nrows() {\n",
    "    for j in 0..A.ncols() {\n",
    "        print!(\"{:5.1} \", A.get(i, j).unwrap());\n",
    "    }\n",
    "    print!(\"   \");\n",
    "    for j in 0..B.ncols() {\n",
    "        print!(\"{:5.1} \", B.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Different array shapes for flexibility demonstration\n",
    "let shapes_demo = [\n",
    "    (\"1x8 (row vector)\", 1, 8),\n",
    "    (\"8x1 (column vector)\", 8, 1),\n",
    "    (\"2x4 (wide matrix)\", 2, 4),\n",
    "    (\"4x2 (tall matrix)\", 4, 2),\n",
    "    (\"3x3 (square matrix)\", 3, 3),\n",
    "];\n",
    "\n",
    "println!(\"\\nArray shape flexibility:\");\n",
    "for (name, rows, cols) in shapes_demo.iter() {\n",
    "    let test_array = ArrayF64::ones(*rows, *cols);\n",
    "    println!(\"  {} -> {}x{} = {} elements\", name, rows, cols, rows * cols);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-optimization",
   "metadata": {},
   "source": [
    "## Performance Optimization Patterns\n",
    "\n",
    "Best practices for high-performance array operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "performance-patterns",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Optimization Patterns:\n",
      "\n",
      "1. Avoiding Multiple Temporary Allocations\n",
      "Multiple temporaries: 204.844µs\n",
      "Single pass with map: 57.562µs\n",
      "\n",
      "2. Memory Access Patterns\n",
      "Row-major access: 27.924µs (sum = 10000)\n",
      "Column-major access: 27.824µs (sum = 10000)\n",
      "Cache-friendly access is 1.0x faster\n",
      "\n",
      "3. Operation Type Comparison\n",
      "Scalar addition (A + 5.0): 102.423µs\n",
      "Array addition (A + B): 111.797µs\n",
      "Complex map (sin(x).exp()): 333.981µs\n",
      "\n",
      "4. Operation Order Optimization\n",
      "Matrix chain: (200x10) × (10x100) × vector(100)\n",
      "Order (A×B)×v: 6.795254ms (creates 200x100 intermediate)\n",
      "Order A×(B×v): 4.234µs (creates vector of length 10)\n",
      "Better ordering is 1604.9x faster\n",
      "\n",
      "Performance Optimization Summary:\n",
      "✓ Use single-pass operations when possible\n",
      "✓ Access memory in row-major order for cache efficiency\n"
     ]
    }
   ],
   "source": [
    "use std::time::Instant;\n",
    "\n",
    "println!(\"Performance Optimization Patterns:\");\n",
    "\n",
    "let size = 100;\n",
    "let A = ArrayF64::ones(size, size);\n",
    "let B = ArrayF64::ones(size, size);\n",
    "\n",
    "// Pattern 1: Multiple temporary allocations vs single operations\n",
    "println!(\"\\n1. Avoiding Multiple Temporary Allocations\");\n",
    "\n",
    "// Multiple temporaries (less efficient)\n",
    "let start = Instant::now();\n",
    "let temp1 = &A + &B;        // First temporary\n",
    "let temp2 = &temp1 * 2.0;   // Second temporary  \n",
    "let _result1 = &temp2 + 1.0; // Third temporary\n",
    "let multiple_temps_time = start.elapsed();\n",
    "println!(\"Multiple temporaries: {:?}\", multiple_temps_time);\n",
    "\n",
    "// Single complex operation (more efficient)\n",
    "let start = Instant::now();\n",
    "let _result2 = A.map(|x| (x + 1.0) * 2.0 + 1.0); // Single pass\n",
    "let single_pass_time = start.elapsed();\n",
    "println!(\"Single pass with map: {:?}\", single_pass_time);\n",
    "\n",
    "// Pattern 2: Memory access patterns\n",
    "println!(\"\\n2. Memory Access Patterns\");\n",
    "\n",
    "let matrix = ArrayF64::ones(size, size);\n",
    "\n",
    "// Row-major access (cache-friendly)\n",
    "let start = Instant::now();\n",
    "let mut row_sum = 0.0;\n",
    "for i in 0..size {\n",
    "    for j in 0..size {\n",
    "        row_sum += matrix.get(i, j).unwrap();\n",
    "    }\n",
    "}\n",
    "let row_time = start.elapsed();\n",
    "println!(\"Row-major access: {:?} (sum = {})\", row_time, row_sum);\n",
    "\n",
    "// Column-major access (less cache-friendly)\n",
    "let start = Instant::now();\n",
    "let mut col_sum = 0.0;\n",
    "for j in 0..size {\n",
    "    for i in 0..size {\n",
    "        col_sum += matrix.get(i, j).unwrap();\n",
    "    }\n",
    "}\n",
    "let col_time = start.elapsed();\n",
    "println!(\"Column-major access: {:?} (sum = {})\", col_time, col_sum);\n",
    "\n",
    "if col_time.as_nanos() > 0 && row_time.as_nanos() > 0 {\n",
    "    let cache_benefit = col_time.as_nanos() as f64 / row_time.as_nanos() as f64;\n",
    "    println!(\"Cache-friendly access is {:.1}x faster\", cache_benefit);\n",
    "}\n",
    "\n",
    "// Pattern 3: Scalar vs array operations efficiency\n",
    "println!(\"\\n3. Operation Type Comparison\");\n",
    "\n",
    "// Scalar operations (very fast)\n",
    "let start = Instant::now();\n",
    "let _scalar_result = &A + 5.0;\n",
    "let scalar_time = start.elapsed();\n",
    "println!(\"Scalar addition (A + 5.0): {:?}\", scalar_time);\n",
    "\n",
    "// Array operations (moderate speed)\n",
    "let start = Instant::now();\n",
    "let _array_result = &A + &B;\n",
    "let array_time = start.elapsed();\n",
    "println!(\"Array addition (A + B): {:?}\", array_time);\n",
    "\n",
    "// Complex element-wise (slower)\n",
    "let start = Instant::now();\n",
    "let _complex_result = A.map(|x| x.sin().exp());\n",
    "let complex_time = start.elapsed();\n",
    "println!(\"Complex map (sin(x).exp()): {:?}\", complex_time);\n",
    "\n",
    "// Pattern 4: Matrix operation ordering\n",
    "println!(\"\\n4. Operation Order Optimization\");\n",
    "\n",
    "// Create matrices for chain multiplication example\n",
    "let tall_matrix = ArrayF64::ones(size * 2, 10);  // (200, 10)\n",
    "let wide_matrix = ArrayF64::ones(10, size);      // (10, 100)\n",
    "let vector = VectorF64::ones(size);              // (100,) as vector\n",
    "\n",
    "println!(\"Matrix chain: ({}x{}) × ({}x{}) × vector({})\", \n",
    "         tall_matrix.nrows(), tall_matrix.ncols(),\n",
    "         wide_matrix.nrows(), wide_matrix.ncols(),\n",
    "         vector.len());\n",
    "\n",
    "// Order 1: (A × B) × v - creates large intermediate matrix\n",
    "let start = Instant::now();\n",
    "let ab = &tall_matrix ^ &wide_matrix;  // (200, 100) - large!\n",
    "let _result_order1 = &ab ^ &vector;     // (200,)\n",
    "let order1_time = start.elapsed();\n",
    "println!(\"Order (A×B)×v: {:?} (creates {}x{} intermediate)\", \n",
    "         order1_time, ab.nrows(), ab.ncols());\n",
    "\n",
    "// Order 2: A × (B × v) - smaller intermediate\n",
    "let start = Instant::now();\n",
    "let bv = &wide_matrix ^ &vector;        // (10,) - small!\n",
    "let _result_order2 = &tall_matrix ^ &bv; // (200,)\n",
    "let order2_time = start.elapsed();\n",
    "println!(\"Order A×(B×v): {:?} (creates vector of length {})\", \n",
    "         order2_time, bv.len());\n",
    "\n",
    "if order1_time.as_nanos() > 0 && order2_time.as_nanos() > 0 {\n",
    "    let ordering_benefit = order1_time.as_nanos() as f64 / order2_time.as_nanos() as f64;\n",
    "    println!(\"Better ordering is {:.1}x faster\", ordering_benefit);\n",
    "}\n",
    "\n",
    "println!(\"\\nPerformance Optimization Summary:\");\n",
    "println!(\"✓ Use single-pass operations when possible\");\n",
    "println!(\"✓ Access memory in row-major order for cache efficiency\");\n",
    "println!(\"✓ Prefer scalar operations when applicable\");\n",
    "println!(\"✓ Order matrix multiplications to minimize intermediate sizes\");\n",
    "println!(\"✓ Use map() for complex element-wise transformations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "real-world-example",
   "metadata": {},
   "source": [
    "## Real-World Example: Image Convolution\n",
    "\n",
    "Demonstrate advanced operations with a practical computer vision example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "image-convolution",
   "metadata": {
    "vscode": {
     "languageId": "rust"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prefer scalar operations when applicable\n",
      "✓ Order matrix multiplications to minimize intermediate sizes\n",
      "✓ Use map() for complex element-wise transformations\n",
      "Real-World Example: Image Processing\n",
      "Created 8x8 synthetic image\n",
      "Original image:\n",
      " 0.10  0.20  0.30  0.40  0.50  0.60  0.70  0.80 \n",
      " 0.20  0.40  0.60  0.80  1.00  0.80  0.60  0.40 \n",
      " 0.30  0.60  0.90  1.00  1.00  0.90  0.60  0.30 \n",
      " 0.40  0.80  1.00  1.00  1.00  1.00  0.80  0.40 \n",
      " 0.50  1.00  1.00  1.00  1.00  1.00  1.00  0.50 \n",
      " 0.60  0.80  0.90  1.00  1.00  0.90  0.80  0.60 \n",
      " 0.70  0.60  0.60  0.80  0.80  0.60  0.60  0.70 \n",
      " 0.80  0.40  0.30  0.40  0.40  0.30  0.40  0.80 \n",
      "\n",
      "Image statistics:\n",
      "  Min: 0.1000\n",
      "  Max: 1.0000\n",
      "  Mean: 0.6750\n",
      "  Total pixels: 64\n",
      "\n",
      "Image Processing Transformations:\n",
      "\n",
      "1. Brightened image (x + 0.2, clamped to [0,1]):\n",
      " 0.30  0.40  0.50  0.60 ...\n",
      " 0.40  0.60  0.80  1.00 ...\n",
      " 0.50  0.80  1.00  1.00 ...\n",
      " 0.60  1.00  1.00  1.00 ...\n",
      "...\n",
      "\n",
      "2. Contrast enhanced ((x-0.5)*1.5+0.5):\n",
      " 0.00  0.05  0.20  0.35 ...\n",
      " 0.05  0.35  0.65  0.95 ...\n",
      " 0.20  0.65  1.00  1.00 ...\n",
      " 0.35  0.95  1.00  1.00 ...\n",
      "...\n",
      "\n",
      "3. Gamma corrected (x^0.7):\n",
      " 0.20  0.32  0.43  0.53 ...\n",
      " 0.32  0.53  0.70  0.86 ...\n",
      " 0.43  0.70  0.93  1.00 ...\n",
      " 0.53  0.86  1.00  1.00 ...\n",
      "...\n",
      "\n",
      "4. Thresholded at 0.5 (binary image):\n",
      "  0   0   0   0   0   1   1   1 \n",
      "  0   0   1   1   1   1   1   0 \n",
      "  0   1   1   1   1   1   1   0 \n",
      "  0   1   1   1   1   1   1   0 \n",
      "  0   1   1   1   1   1   1   0 \n",
      "  1   1   1   1   1   1   1   1 \n",
      "  1   1   1   1   1   1   1   1 \n",
      "  1   0   0   0   0   0   0   1 \n",
      "\n",
      "5. Simple edge detection (horizontal differences):\n",
      " 0.10  0.10  0.10  0.10  0.10  0.10  0.10 \n",
      " 0.20  0.20  0.20  0.20  0.20  0.20  0.20 \n",
      " 0.30  0.30  0.10  0.00  0.10  0.30  0.30 \n",
      " 0.40  0.20  0.00  0.00  0.00  0.20  0.40 \n",
      " 0.50  0.00  0.00  0.00  0.00  0.00  0.50 \n",
      " 0.20  0.10  0.10  0.00  0.10  0.10  0.20 \n",
      " 0.10  0.00  0.20  0.00  0.20  0.00  0.10 \n",
      " 0.40  0.10  0.10  0.00  0.10  0.10  0.40 \n",
      "\n",
      "6. Processing pipeline (brightness → gamma → contrast):\n",
      " 0.23  0.36  0.48  0.59 ...\n",
      " 0.36  0.59  0.80  1.00 ...\n",
      " 0.48  0.80  1.00  1.00 ...\n",
      " 0.59  1.00  1.00  1.00 ...\n",
      "...\n",
      "\n",
      "Image Processing Summary:\n",
      "✓ Brightness adjustment with clamping\n",
      "✓ Contrast enhancement with linear scaling\n",
      "✓ Gamma correction for tone mapping\n",
      "✓ Binary thresholding for segmentation\n",
      "✓ Simple edge detection via differences\n",
      "✓ Multi-step processing pipelines\n"
     ]
    }
   ],
   "source": [
    "println!(\"Real-World Example: Image Processing\");\n",
    "\n",
    "// Create a synthetic \"image\" (grayscale)\n",
    "let img_size = 8;  // Using smaller size for clear output\n",
    "let test_image = array64![\n",
    "    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "    [0.2, 0.4, 0.6, 0.8, 1.0, 0.8, 0.6, 0.4],\n",
    "    [0.3, 0.6, 0.9, 1.0, 1.0, 0.9, 0.6, 0.3],\n",
    "    [0.4, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4],\n",
    "    [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5],\n",
    "    [0.6, 0.8, 0.9, 1.0, 1.0, 0.9, 0.8, 0.6],\n",
    "    [0.7, 0.6, 0.6, 0.8, 0.8, 0.6, 0.6, 0.7],\n",
    "    [0.8, 0.4, 0.3, 0.4, 0.4, 0.3, 0.4, 0.8]\n",
    "];\n",
    "\n",
    "println!(\"Created {}x{} synthetic image\", img_size, img_size);\n",
    "println!(\"Original image:\");\n",
    "for i in 0..test_image.nrows() {\n",
    "    for j in 0..test_image.ncols() {\n",
    "        print!(\"{:5.2} \", test_image.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Calculate basic statistics manually\n",
    "let mut min_val = f64::MAX;\n",
    "let mut max_val = f64::MIN;\n",
    "let mut sum_val = 0.0;\n",
    "let mut count = 0;\n",
    "\n",
    "for i in 0..test_image.nrows() {\n",
    "    for j in 0..test_image.ncols() {\n",
    "        let val = test_image.get(i, j).unwrap();\n",
    "        min_val = min_val.min(val);\n",
    "        max_val = max_val.max(val);\n",
    "        sum_val += val;\n",
    "        count += 1;\n",
    "    }\n",
    "}\n",
    "let mean_val = sum_val / count as f64;\n",
    "\n",
    "println!(\"\\nImage statistics:\");\n",
    "println!(\"  Min: {:.4}\", min_val);\n",
    "println!(\"  Max: {:.4}\", max_val);\n",
    "println!(\"  Mean: {:.4}\", mean_val);\n",
    "println!(\"  Total pixels: {}\", count);\n",
    "\n",
    "// Image processing transformations using map()\n",
    "println!(\"\\nImage Processing Transformations:\");\n",
    "\n",
    "// 1. Brightness adjustment\n",
    "let brightened = test_image.map(|x| (x + 0.2).clamp(0.0, 1.0));\n",
    "println!(\"\\n1. Brightened image (x + 0.2, clamped to [0,1]):\");\n",
    "for i in 0..brightened.nrows().min(4) {\n",
    "    for j in 0..brightened.ncols().min(4) {\n",
    "        print!(\"{:5.2} \", brightened.get(i, j).unwrap());\n",
    "    }\n",
    "    println!(\"...\");\n",
    "}\n",
    "println!(\"...\");\n",
    "\n",
    "// 2. Contrast enhancement\n",
    "let contrasted = test_image.map(|x| ((x - 0.5) * 1.5 + 0.5).clamp(0.0, 1.0));\n",
    "println!(\"\\n2. Contrast enhanced ((x-0.5)*1.5+0.5):\");\n",
    "for i in 0..contrasted.nrows().min(4) {\n",
    "    for j in 0..contrasted.ncols().min(4) {\n",
    "        print!(\"{:5.2} \", contrasted.get(i, j).unwrap());\n",
    "    }\n",
    "    println!(\"...\");\n",
    "}\n",
    "println!(\"...\");\n",
    "\n",
    "// 3. Gamma correction\n",
    "let gamma_corrected = test_image.map(|x| x.powf(0.7));\n",
    "println!(\"\\n3. Gamma corrected (x^0.7):\");\n",
    "for i in 0..gamma_corrected.nrows().min(4) {\n",
    "    for j in 0..gamma_corrected.ncols().min(4) {\n",
    "        print!(\"{:5.2} \", gamma_corrected.get(i, j).unwrap());\n",
    "    }\n",
    "    println!(\"...\");\n",
    "}\n",
    "println!(\"...\");\n",
    "\n",
    "// 4. Thresholding (binary conversion)\n",
    "let threshold = 0.5;\n",
    "let thresholded = test_image.map(|x| if x > threshold { 1.0 } else { 0.0 });\n",
    "println!(\"\\n4. Thresholded at {} (binary image):\", threshold);\n",
    "for i in 0..thresholded.nrows() {\n",
    "    for j in 0..thresholded.ncols() {\n",
    "        print!(\"{:3.0} \", thresholded.get(i, j).unwrap());\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// 5. Edge detection simulation (simple difference)\n",
    "println!(\"\\n5. Simple edge detection (horizontal differences):\");\n",
    "for i in 0..test_image.nrows() {\n",
    "    for j in 0..(test_image.ncols()-1) {\n",
    "        let curr = test_image.get(i, j).unwrap();\n",
    "        let next = test_image.get(i, j+1).unwrap();\n",
    "        let diff = (next - curr).abs();\n",
    "        print!(\"{:5.2} \", diff);\n",
    "    }\n",
    "    println!();\n",
    "}\n",
    "\n",
    "// Image processing pipeline\n",
    "let pipeline_result = test_image\n",
    "    .map(|x| (x + 0.1).clamp(0.0, 1.0))  // Brightness\n",
    "    .map(|x| x.powf(0.8))                 // Gamma\n",
    "    .map(|x| ((x - 0.5) * 1.2 + 0.5).clamp(0.0, 1.0)); // Contrast\n",
    "\n",
    "println!(\"\\n6. Processing pipeline (brightness → gamma → contrast):\");\n",
    "for i in 0..pipeline_result.nrows().min(4) {\n",
    "    for j in 0..pipeline_result.ncols().min(4) {\n",
    "        print!(\"{:5.2} \", pipeline_result.get(i, j).unwrap());\n",
    "    }\n",
    "    println!(\"...\");\n",
    "}\n",
    "println!(\"...\");\n",
    "\n",
    "println!(\"\\nImage Processing Summary:\");\n",
    "println!(\"✓ Brightness adjustment with clamping\");\n",
    "println!(\"✓ Contrast enhancement with linear scaling\");\n",
    "println!(\"✓ Gamma correction for tone mapping\");\n",
    "println!(\"✓ Binary thresholding for segmentation\");\n",
    "println!(\"✓ Simple edge detection via differences\");\n",
    "println!(\"✓ Multi-step processing pipelines\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered broadcasting and advanced array operations in rustlab-math:\n",
    "\n",
    "### Key Broadcasting Concepts:\n",
    "- **Scalar Broadcasting**: Arrays can be combined with scalars (e.g., `array + 10.0`)\n",
    "- **Same-Shape Operations**: Arrays of identical dimensions work seamlessly\n",
    "- **Element-wise Transformations**: Use `map()` for complex per-element operations\n",
    "- **Memory Efficiency**: Operations are optimized for performance\n",
    "\n",
    "### Advanced Techniques Learned:\n",
    "- **Element Access**: Safe indexing with `get(i, j).unwrap()`\n",
    "- **Shape Manipulation**: Transposition and different array layouts\n",
    "- **Performance Patterns**: Cache-friendly access, operation ordering\n",
    "- **Vectorized Operations**: Using `map()` for mathematical functions\n",
    "- **Real-world Applications**: Image processing, feature standardization\n",
    "\n",
    "### Current Broadcasting Support:\n",
    "✅ **Scalar operations**: `array + scalar`, `array * scalar`\n",
    "✅ **Same-shape arrays**: `array1 + array2` (when dimensions match)\n",
    "✅ **Element-wise functions**: `map()` with mathematical operations\n",
    "✅ **Matrix operations**: Clear distinction between `*` (element-wise) and `^` (matrix multiplication)\n",
    "\n",
    "⚠️  **Limited**: True NumPy-style broadcasting between different shapes not yet implemented\n",
    "\n",
    "### Performance Guidelines:\n",
    "1. **Use scalar operations** when possible (fastest)\n",
    "2. **Access arrays row-major** for cache efficiency  \n",
    "3. **Chain operations with map()** to avoid temporaries\n",
    "4. **Order matrix multiplications** to minimize intermediate sizes\n",
    "5. **Prefer same-shape operations** over mixed dimensions\n",
    "\n",
    "### Key Methods Available:\n",
    "- **Creation**: `zeros()`, `ones()`, `array64![]`\n",
    "- **Element-wise**: `map()`, `map_with_index()`\n",
    "- **Arithmetic**: `+`, `-`, `*` (scalar and element-wise), `^` (matrix multiplication)\n",
    "- **Transformations**: `transpose()`, `clamp()`\n",
    "- **Access**: `get()`, `nrows()`, `ncols()`\n",
    "\n",
    "### Practical Applications Demonstrated:\n",
    "- Image processing pipelines (brightness, contrast, gamma correction)\n",
    "- Feature standardization for machine learning\n",
    "- Softmax function with numerical stability\n",
    "- Performance optimization techniques\n",
    "\n",
    "**Next**: Mathematical functions and constants →"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
